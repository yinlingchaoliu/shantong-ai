{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e7e935-0e8d-4b47-bc3f-bebbbd2268d4",
   "metadata": {},
   "source": [
    "## 十、Hugging Face 简介\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>注意：</b> 今天的代码，都不要在Jupyter笔记上直接运行，会死机！！请下载左边的脚本，在自己的环境里运行。\n",
    "</div>\n",
    "\n",
    "### 10.1、Hugging Face 是什么\n",
    "\n",
    "- 官网：http://www.huggingface.co\n",
    "- 相当于面向 NLP 模型的 Github\n",
    "- 尤其基于 transformer 的开源模型非常全\n",
    "- 封装了模型、数据集、训练器等，使模型的下载、使用、训练都非常方便\n",
    "\n",
    "### 10.2、Hugging Face 安装\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29644189-6349-42f2-9e5f-56c5911df05b",
   "metadata": {},
   "source": [
    "```python\n",
    "# pip安装\n",
    "pip install transformers # 安装最新的版本\n",
    "pip install transformers == 4.30 # 安装指定版本\n",
    "# conda安装\n",
    "conda install -c huggingface transformers  # 只4.0以后的版本\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922379e0-0578-4233-b534-94b3aa87ebd1",
   "metadata": {},
   "source": [
    "### 10.3、加载一个预训练的模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e21882-1917-4b4b-a5ed-30f1042eb96c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "\n",
    "# 运行: python load-and-run.py\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# model_name_or_path=\"baichuan-inc/baichuan-7B\"\n",
    "model_name_or_path = \"distilgpt2\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path, trust_remote_code=True\n",
    ").to(device)\n",
    "\n",
    "inputs = tokenizer('I am', return_tensors='pt').to(device)\n",
    "\n",
    "pred = model.generate(**inputs, max_new_tokens=10)\n",
    "\n",
    "output = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)\n",
    "\n",
    "print(output)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40341469-870e-4ab6-b0b0-ec628e9329da",
   "metadata": {},
   "source": [
    "### 10.4、基于 Huggingface 训练/微调一个模型\n",
    "\n",
    "- 定义你的 Tokenizer\n",
    "- 定义你的模型结构：如果是已知模型结构，可以通过 Config 指定其超参（层数，头数，维度等）\n",
    "- 定义数据集加载器：分别加载 train、validation、test 数据集\n",
    "- 训练你的 Tokenizer **（可选）**：如果你完全从 0 开始训练一个模型，这步是必须的\n",
    "- 定义一个数据处理函数：Tokenizer 一般用在这里，把原始数据处理成满足模型输入的 Tensor 形式\n",
    "- 定义 TrainingArguments：模型训练的各种超参在这里指定\n",
    "- 定义一个 Trainer\n",
    "- 定义 Evaluation Metrics **（可选）**：如果你希望观察除 loss 外的测试指标\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a704a1-2998-424e-a3db-bacefa417964",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# 运行: python fine-tune-mrpc.py\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from datasets import load_metric\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "import transformers\n",
    "from transformers import DataCollatorWithPadding\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "SEED=42\n",
    "\n",
    "# ALBERT是一种压缩过的BERT\n",
    "MODEL_NAME = \"albert-base-v2\"\n",
    "DATASET_NAME = \"glue\" # 一组NLP评测任务\n",
    "DATASET_TASK = \"mrpc\" # MRPC 是其中一个子任务 -- Microsoft Research Paraphrase Corpus\n",
    "\n",
    "# 在Bert的基础上加了一个线性分类器\n",
    "class MyClassifier(torch.nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super().__init__()\n",
    "        self.bert_encoder = backbone\n",
    "        self.linear = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def compute_loss(self, logits, labels):\n",
    "        loss_fct = nn.CrossEntropyLoss()\n",
    "        return loss_fct(logits, labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,labels=None):\n",
    "        output = self.bert_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = output.last_hidden_state[:, 0, :]\n",
    "        output = self.linear(output)\n",
    "        if labels is not None:\n",
    "            loss = self.compute_loss(output, labels)\n",
    "            return loss, output\n",
    "        return output\n",
    "\n",
    "# 加载数据集对应的评估方法\n",
    "glue_metric = datasets.load_metric(DATASET_NAME, DATASET_TASK)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return glue_metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# 加载数据集\n",
    "raw_datasets = load_dataset(DATASET_NAME,DATASET_TASK)\n",
    "\n",
    "# 训练集\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "# 验证集\n",
    "raw_valid_dataset = raw_datasets[\"validation\"]\n",
    "\n",
    "columns = raw_train_dataset.column_names\n",
    "\n",
    "# 设置随机种子\n",
    "transformers.set_seed(SEED)\n",
    "\n",
    "# 定义tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 定义数据处理函数，把原始数据转成input_ids, attention_mask, labels\n",
    "def process_fn(examples):\n",
    "    inputs = tokenizer(examples[\"sentence1\"], examples[\"sentence2\"], truncation=True, max_length=128)\n",
    "    examples[\"input_ids\"] = inputs[\"input_ids\"]\n",
    "    examples[\"attention_mask\"] = inputs[\"attention_mask\"]\n",
    "    examples[\"labels\"] = examples[\"label\"]\n",
    "    return examples\n",
    "\n",
    "\n",
    "\n",
    "tokenized_train_dataset = raw_train_dataset.map(\n",
    "    process_fn,\n",
    "    batched=True,\n",
    "    remove_columns=columns\n",
    ")\n",
    "\n",
    "tokenized_valid_dataset = raw_valid_dataset.map(\n",
    "    process_fn,\n",
    "    batched=True,\n",
    "    remove_columns=columns\n",
    ")\n",
    "\n",
    "\n",
    "# 定义数据校准器（自动生成batch）\n",
    "collater = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer, return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "# 定义模型 -- 其实Transformer可以直接用AutoModelForSequenceClassification\n",
    "#model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "# 我手工写了分类器层，为了方便大家理解什么叫在Transformer上面做分类任务\n",
    "backbone = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model = MyClassifier(backbone)\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",        # checkpoint保存路径\n",
    "    evaluation_strategy=\"steps\",    # 每N步做一次eval\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # 训练epoch数\n",
    "    per_device_train_batch_size=8,  # 每张卡的batch大小\n",
    "    gradient_accumulation_steps=4,   # 累加几个step做一次参数更新\n",
    "    per_device_eval_batch_size=8,  # evaluation batch size\n",
    "    logging_steps=20,             # 每20步eval一次\n",
    "    save_steps=20,                # 每20步保存一个checkpoint\n",
    "    learning_rate=2e-5,             # 学习率\n",
    "    warmup_ratio=0.1,               # 预热（可选）\n",
    ")\n",
    "\n",
    "# 定义训练器\n",
    "trainer = Trainer(\n",
    "    model=model, # 待训练模型\n",
    "    args=training_args, # 训练参数\n",
    "    data_collator=collater, # 数据校准器\n",
    "    train_dataset=tokenized_train_dataset, # 训练集\n",
    "    eval_dataset=tokenized_valid_dataset, # 验证集\n",
    "    compute_metrics=compute_metrics, # 评价指标\n",
    ")\n",
    "\n",
    "# 禁用wandb（与huggingface.co同步的机制）\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\r\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d63ee6-173a-44c4-8e5b-d47f09fe188b",
   "metadata": {},
   "source": [
    "### 10.5、从头训练一个的模型（参考）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b231d6de-243b-4568-8a92-b45b40befa26",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "import torch.distributed\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from transformers import GPT2TokenizerFast, DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import multiprocessing\n",
    "import random\n",
    "\n",
    "MODEL_NAME = \"gpt2\"\n",
    "SEED = 42\n",
    "MAX_LENGTH = 1024\n",
    "MIN_LENGTH = 4\n",
    "VOCAB_SIZE = 50257\n",
    "\n",
    "# 定义数据处理函数\n",
    "\n",
    "\n",
    "def prepare_train_features(examples):\n",
    "    # 略过空行\n",
    "    examples[\"nonempty_text\"] = [\n",
    "        d.strip() for d in examples[\"text\"] if len(d.strip()) > 0\n",
    "    ]\n",
    "\n",
    "    # Convert the tokens into ids using the trained tokenizer\n",
    "\n",
    "    tokenized_example = tokenizer(\n",
    "        examples[\"nonempty_text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH*100,\n",
    "    )\n",
    "\n",
    "    # 模型输出的字段\n",
    "    examples[\"input_ids\"] = []\n",
    "    examples[\"attention_mask\"] = []\n",
    "\n",
    "    del examples[\"text\"]\n",
    "    del examples[\"nonempty_text\"]\n",
    "\n",
    "    for input_ids, attention_mask in zip(tokenized_example[\"input_ids\"], tokenized_example[\"attention_mask\"]):\n",
    "\n",
    "        trunc_ids = input_ids[:min(len(input_ids), MAX_LENGTH)]\n",
    "        trunc_mask = attention_mask[:min(len(attention_mask), MAX_LENGTH)]\n",
    "\n",
    "        # 把长句切成MAX_LENGTH长度的片段, 最后一段如果小于MIN_LENGTH则忽略\n",
    "        while len(trunc_ids) > MIN_LENGTH:\n",
    "            trunc_len = len(trunc_ids)\n",
    "            if trunc_len < MAX_LENGTH:\n",
    "                examples[\"input_ids\"].append(\n",
    "                    trunc_ids+[tokenizer.pad_token_id]*(MAX_LENGTH-trunc_len))\n",
    "                examples[\"attention_mask\"].append(\n",
    "                    trunc_mask+[0]*(MAX_LENGTH-trunc_len))\n",
    "            else:\n",
    "                examples[\"input_ids\"].append(trunc_ids)\n",
    "                examples[\"attention_mask\"].append(trunc_mask)\n",
    "\n",
    "            input_ids = input_ids[trunc_len:]\n",
    "            attention_mask = attention_mask[trunc_len:]\n",
    "\n",
    "            trunc_ids = input_ids[:min(len(input_ids), MAX_LENGTH)]\n",
    "            trunc_mask = attention_mask[:min(len(attention_mask), MAX_LENGTH)]\n",
    "\n",
    "    examples['labels'] = examples['input_ids'].copy()\n",
    "\n",
    "    return examples\n",
    "\n",
    "\n",
    "# 开启多开训练模式\n",
    "torch.distributed.init_process_group(\n",
    "    backend='nccl', init_method=\"env://\", rank=args.local_rank, world_size=args.word_size)\n",
    "torch.cuda.set_device(args.local_rank)\n",
    "\n",
    "# 自动下载openwebtext数据集，展开前几十GB，展开成arrow格式大约500G\n",
    "raw_datasets = load_dataset(\"openwebtext\", split=\"train\")\n",
    "# 这里只用1%的数据作为测试集，否则每次dev时间很长\n",
    "raw_datasets = raw_datasets.train_test_split(test_size=0.01)\n",
    "\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_valid_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "transformers.set_seed(args.seed)\n",
    "\n",
    "# 定义tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "# 获取CPU核数（用于数据加载线程数）\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "if torch.distributed.get_rank() > 0:\n",
    "    # 主进程加载数据，其它进程等待从缓存加载arrow文件\"\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "tokenized_train_dataset = raw_train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=num_proc\n",
    ")\n",
    "\n",
    "tokenized_valid_dataset = raw_valid_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=num_proc\n",
    ")\n",
    "\n",
    "if torch.distributed.get_rank() == 0:\n",
    "    # 主进程加载数据结束\n",
    "    torch.distributed.barrier()\n",
    "\n",
    "# 定义数据校准器（自动生成batch）\n",
    "collater = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# 定义GPT-2模型config\n",
    "model_config = GPT2Config(vocab_size=VOCAB_SIZE,\n",
    "                          max_position_embeddings=MAX_LENGTH, return_dict=True)\n",
    "# 定义模型（此处参数随机初始化）\n",
    "model = GPT2LMHeadModel(config=model_config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./my_model\",        # checkpoint保存路径\n",
    "    evaluation_strategy=\"steps\",    # 每N步做一次eval\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,             # 训练epoch数\n",
    "    per_device_train_batch_size=8,  # 每张卡的batch大小\n",
    "    gradient_accumulation_steps=20,   # 累加几个step做一次参数更新\n",
    "    per_device_eval_batch_size=16,  # evaluation batch size\n",
    "    logging_steps=1000,             # 每1000步eval一次\n",
    "    save_steps=1000,                # 每1000步保存一个checkpoint\n",
    "    learning_rate=1e-3,             # 学习率\n",
    "    warmup_steps=2000,              # 预热（可选）\n",
    "    optim=\"adamw_hf\",               # 求解器（默认）\n",
    ")\n",
    "\n",
    "# 定义训练器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collater,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_valid_dataset,\n",
    ")\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 开始训练\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd7a2d-699a-4df0-8721-668c3087fb99",
   "metadata": {},
   "source": [
    "### 10.6、从头训练一个 Tokenizer（参考）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea71f6d-5e98-4a50-a686-8bb0fb5bd5af",
   "metadata": {},
   "source": [
    "```python\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2TokenizerFast\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "VOCAB_SIZE = 50257\n",
    "MAX_LENGTH = 1024\n",
    "SEED = 42\n",
    "MODEL_NAME = \"gpt-2\"\n",
    "\n",
    "raw_datasets = load_dataset(\"openwebtext\", split=\"train\")\n",
    "raw_datasets = raw_datasets.train_test_split(test_size=0.01)\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_valid_dataset = raw_datasets[\"test\"]\n",
    "\n",
    "# 定义一个批量加载数据的迭代器\n",
    "\n",
    "\n",
    "def batch_iterator(batch_size=10000):\n",
    "    for i in tqdm(range(0, len(raw_train_dataset), batch_size)):\n",
    "        yield raw_train_dataset[i: i + batch_size][\"text\"]\n",
    "\n",
    "\n",
    "# 加载预训练的tokenizer（为了复用其定义的特殊token）\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "gpt_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    text_iterator=batch_iterator(), vocab_size=VOCAB_SIZE)\n",
    "gpt_tokenizer.save_pretrained(\"my-tokenizer-\"+MODEL_NAME)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bdfcc2-d6a9-4961-8e9b-27ffdd243d2d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>划重点：</b> 参数微调过程与上述Training过程基本是一致的。\n",
    "</div>\n",
    "\n",
    "- 定义微调数据集加载器\n",
    "- 定义数据处理函数\n",
    "- 加载预训练模型：AutoModel.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "- 在预训练模型上增加任务相关输出层 **（如果需要）**\n",
    "- 加载预训练 Tokenizer：AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "- 定义各种超参\n",
    "- 定义 Trainer\n",
    "- 定义 Evaluation Metric\n",
    "- 开始训练\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97414a25",
   "metadata": {},
   "source": [
    "[继续](../peft/index.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
