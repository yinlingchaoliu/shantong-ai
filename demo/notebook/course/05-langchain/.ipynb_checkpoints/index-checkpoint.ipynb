{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 这节课会带给你\n",
    "\n",
    "1. 如何使用 LangChain：一套在大模型能力上封装的工具框架\n",
    "2. 如何用几行代码实现一个复杂的 AI 应用\n",
    "3. 面向大模型的流程开发的过程抽象\n",
    "\n",
    "开始上课！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 写在前面\n",
    "\n",
    "- LangChain 是一套面向大模型的开发框架\n",
    "- LangChain 是 AGI 时代软件工程的一个探索和原型\n",
    "- LangChain 并不完美，还在不断迭代中：我写这个课件的时候是 V0.0.200，现在是 V0.0.241\n",
    "- 学习 LangChain 更重要的是借鉴其思想，具体的接口可能很快就会改变\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangChain 的核心组件\n",
    "\n",
    "1. 模型 I/O 封装\n",
    "   - LLMs：大语言模型\n",
    "   - Chat Models：一般基于 LLMs，但按对话结构重新封装\n",
    "   - PromptTemple：提示词模板\n",
    "   - OutputParser：解析输出\n",
    "2. 数据连接封装\n",
    "   - Document Loaders：各种格式文件的加载器\n",
    "   - Document transformers：对文档的常用操作，如：split, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Models：文本向量化表示，用于检索等操作（啥意思？别急，后面详细讲）\n",
    "   - Verctor stores: （面向检索的）向量的存储\n",
    "   - Retrievers: 向量的检索\n",
    "3. 记忆封装\n",
    "   - Memory：这里不是物理内存，从文本的角度，可以理解为“上文”、“历史记录”或者说“记忆力”的管理\n",
    "4. 架构封装\n",
    "   - Chain：实现一个功能或者一系列顺序功能组合\n",
    "   - Agent：根据用户输入，自动规划执行步骤，自动选择每步需要的工具，最终完成用户指定的功能\n",
    "     - Tools：调用外部功能的函数，例如：调 google 搜索、文件 I/O、Linux Shell 等等\n",
    "     - Toolkits：操作某软件的一组工具集，例如：操作 DB、操作 Gmail 等等\n",
    "5. Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、模型 I/O 封装\n",
    "\n",
    "### 1.1 模型 API：LLM vs. ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain == 0.0.240rc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'使用我的产品！\\n\\n你可以在我们的网站上找到有关我们产品的详细信息，包括产品功能、价格、评论等等。我们还提供了专业的客户服务，可以为您提供有关产品的帮助和咨询。我们也提供免费的技术支持服务，如果您在使用我们的产品时遇到任何问题，可以随时联'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()  # 默认是text-davinci-003模型\n",
    "llm.predict(\"你好，欢迎\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你好！感谢您的欢迎。有什么我可以帮助您的吗？'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOpenAI() #默认是gpt-3.5-turbo\n",
    "chat_model.predict(\"你好，欢迎\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='欢迎来到AGIClass！请问你是第一次来上课吗？', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是AGIClass的课程助理。\"),\n",
    "    HumanMessage(content=\"我来上课了\")\n",
    "]\n",
    "chat_model(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型的输入与输出\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject']\n",
      "给我讲个关于小明的笑话\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"给我讲个关于{subject}的笑话\")\n",
    "print(template.input_variables)\n",
    "print(template.format(subject='小明'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>把Prompt模板看作带有参数的函数，下面的内容可能更好理解\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>作业：</b>自学ChatPromptTemplate的使用\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"command\": {\"title\": \"Command\", \"description\": \"linux shell命令名\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"description\": \"命令的参数 (name:value)\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}}, \"required\": [\"command\", \"arguments\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "将用户的指令转换成linux命令.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"command\": {\"title\": \"Command\", \"description\": \"linux shell命令名\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"description\": \"命令的参数 (name:value)\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}}, \"required\": [\"command\", \"arguments\"]}\n",
      "```\n",
      "将系统日期设为2023-04-01\n",
      "====Output=====\n",
      "{\n",
      "  \"command\": \"date\",\n",
      "  \"arguments\": {\n",
      "    \"-s\": \"2023-04-01\"\n",
      "  }\n",
      "}\n",
      "====Parsed=====\n",
      "command='date' arguments={'-s': '2023-04-01'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "\n",
    "def chinese_friendly(string):\n",
    "    lines = string.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('{') and line.endswith('}'):\n",
    "            try:\n",
    "                lines[i] = json.dumps(json.loads(line), ensure_ascii=False)\n",
    "            except:\n",
    "                pass\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "model_name = 'gpt-4'\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# 定义你的输出格式\n",
    "\n",
    "\n",
    "class Command(BaseModel):\n",
    "    command: str = Field(description=\"linux shell命令名\")\n",
    "    arguments: Dict[str, str] = Field(description=\"命令的参数 (name:value)\")\n",
    "\n",
    "    # 你可以添加自定义的校验机制\n",
    "    @validator('command')\n",
    "    def no_space(cls, field):\n",
    "        if \" \" in field or \"\\t\" in field or \"\\n\" in field:\n",
    "            raise ValueError(\"命令名中不能包含空格或回车!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Command)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"将用户的指令转换成linux命令.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(chinese_friendly(parser.get_format_instructions()))\n",
    "\n",
    "\n",
    "query = \"将系统日期设为2023-04-01\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "#print(parser.get_format_instructions())\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(chinese_friendly(model_input.to_string()))\n",
    "\n",
    "output = model(model_input.to_string())\n",
    "print(\"====Output=====\")\n",
    "print(output)\n",
    "print(\"====Parsed=====\")\n",
    "cmd = parser.parse(output)\n",
    "print(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、数据连接封装\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 2.1 文档加载器：Document Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ChatGPT ? \n",
      "Md. Sakibul Islam Sakib  \n",
      " \n",
      "ChatGPT is a conversational language model developed by OpenAI. It is part of the GPT (Generative \n",
      "Pretrained  Transformer) family of models, which are based on the Transformer architecture and trained on \n",
      "vast amounts of text data to generate human -like text.  \n",
      "ChatGPT is designed to generate text in response to an input prompt, making it well suited for \n",
      "conversatio nal applications such as chatbots, customer service agents, and virtual assistants. The model has \n",
      "been trained on a diverse range of conversational data, including websites, books, and social media, \n",
      "allowing it to generate text that is coherent, contextual ly relevant, and often similar to text produced by \n",
      "humans.  \n",
      "To use ChatGPT, a user provides an input prompt, such as a question or statement, which is then fed into \n",
      "the model. The model then generates a response based on its understanding of the input and i ts training \n",
      "data. The model can generate multiple responses for a single input, and the responses can vary in length, \n",
      "style, and content depending on the context of the input.  \n",
      "One of the key strengths of ChatGPT is its ability to generate text that is cont extually relevant to the input \n",
      "prompt. For example, if a user asks a question about the weather, ChatGPT can generate a response that \n",
      "includes relevant information about the weather, such as temperature, precipitation, and wind conditions. \n",
      "If the user then  asks a follow -up question, ChatGPT can use the previous conversation as context to generate \n",
      "a response that is relevant to the previous conversation.  \n",
      "ChatGPT can also generate text that is coherent and flows well, allowing it to participate in longer \n",
      "conv ersations. The model has been trained on a large amount of conversational data, and has learned how \n",
      "to generate text that is grammatically correct, uses appropriate vocabulary and tone, and is coherent with \n",
      "the input prompt and the overall conversation.  \n",
      "In addition to generating text, ChatGPT can also be used for other NLP tasks such as question answering, \n",
      "summarization, and text classification. The model has been fine -tuned on specific tasks, allowing it to \n",
      "perform well on these tasks while still retaining  its ability to generate human -like text.  \n",
      "ChatGPT is part of a larger trend of using large language models for conversational applications. These \n",
      "models have the potential to revolutionize the way we interact with technology, allowing us to interact with \n",
      "devices and services in a more natural and intuitive way. However, it is important to note that these models \n",
      "are not perfect, and may sometimes generate text that is irrelevant, offensive, or factually incorrect.  \n",
      " \n",
      "In conclusion, ChatGPT is a powerful conver sational language model that is capable of generating human -\n",
      "like text in response to an input prompt. It has a wide range of potential applications, from chatbots and \n",
      "virtual assistants to NLP tasks such as question answering and text classification. Howev er, it is important \n",
      "to carefully consider the limitations and potential risks associated with these models, and to use them \n",
      "responsibly.  \n",
      "View publication stats\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"WhatisChatGPT.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 文档处理器\n",
    "\n",
    "例 1：TextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ChatGPT ? \n",
      "Md. Sakibul Islam Sakib\n",
      "-------\n",
      "ChatGPT is a conversational language model\n",
      "-------\n",
      "model developed by OpenAI. It is part of the GPT\n",
      "-------\n",
      "the GPT (Generative\n",
      "-------\n",
      "Pretrained  Transformer) family of models, which\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,  # 思考：为什么要做overlap\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs[:5]:\n",
    "    print(para.page_content)\n",
    "    print('-------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "例 2：Doctran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install doctran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPT是由OpenAI开发的一种对话语言模型。它是GPT（生成预训练变压器）模型系列的一部分，基于变压器架构，并在大量文本数据上进行训练，以生成类似人类的文本。ChatGPT旨在根据输入提示生成文本，非常适用于聊天机器人、客服代理和虚拟助手等对话应用。该模型经过多样化的对话数据训练，包括网站、书籍和社交媒体，使其能够生成连贯、上下文相关且通常类似于人类生成的文本。用户提供一个输入提示，例如问题或陈述，然后将其输入模型。模型根据对输入的理解和训练数据生成响应。模型可以为单个输入生成多个响应，响应的长度、风格和内容可以根据输入的上下文而变化。ChatGPT的一个关键优势是其能够生成与输入提示相关的文本。例如，如果用户询问天气问题，ChatGPT可以生成包含与天气相关的信息（如温度、降水和风况）的响应。如果用户随后提出跟进问题，ChatGPT可以使用先前的对话作为上下文，生成与先前对话相关的响应。ChatGPT还可以生成连贯流畅的文本，使其能够参与更长的对话。该模型经过大量对话数据的训练，已经学会了如何生成语法正确、使用适当的词汇和语气，并与输入提示和整体对话一致的文本。除了生成文本，ChatGPT还可以用于其他自然语言处理任务，如问答、摘要和文本分类。该模型经过特定任务的微调，使其在这些任务上表现良好，同时仍保留生成类似人类文本的能力。ChatGPT是使用大型语言模型进行对话应用的一个重要趋势的一部分。这些模型有潜力彻底改变我们与技术互动的方式，使我们能够以更自然、更直观的方式与设备和服务进行交互。然而，需要注意的是，这些模型并不完美，有时可能生成与主题无关、冒犯性或事实错误的文本。总之，ChatGPT是一个强大的对话语言模型，能够根据输入提示生成类似人类的文本。它具有广泛的潜在应用，从聊天机器人和虚拟助手到问答和文本分类等自然语言处理任务。然而，需要仔细考虑这些模型的限制和潜在风险，并负责任地使用它们。\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import DoctranTextTranslator\n",
    "\n",
    "translator = DoctranTextTranslator(\n",
    "    openai_api_model=\"gpt-3.5-turbo\", language=\"chinese\")\n",
    "\n",
    "translated_document = await translator.atransform_documents(pages)\n",
    "\n",
    "print(translated_document[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 文档向量化：Text Embeddings\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Embedding：</b>将目标物体（词、句子、文章）表示成向量的方法\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.011436891742050648, -0.012987656518816948, 0.00902028288692236, -0.01197319757193327, -0.02477347105741501, 0.01448672916740179, -0.021891633048653603, -0.005188601091504097, -0.0012567657977342606, -0.03370329365134239]\n",
      "1536\n",
      "[-0.0026156024105142475, 0.0007669371424757893, -0.0046187359068776655, -0.005695960389375307, -0.010613725343519495, 0.026026324865937287, -0.014497498739680675, -0.001994126675894642, -0.014295743539741444, -0.018316421352086065]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text = \"这是一个测试\"\n",
    "document = \"测试文档\"\n",
    "query_vec = embeddings.embed_query(text)\n",
    "doc_vec = embeddings.embed_documents([document])\n",
    "\n",
    "print(len(query_vec))\n",
    "print(query_vec[:10])  # 为了展示方便，只打印前10维\n",
    "print(len(doc_vec[0]))\n",
    "print(doc_vec[0][:10])  # 为了展示方便，只打印前10维\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 向量的存储（与索引）：Vectorstores\n",
    "\n",
    "<img src=\"vector_stores.jpg\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the key strengths of ChatGPT is its ability to generate text that is cont extually relevant to the input\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(paragraphs, embeddings)\n",
    "\n",
    "query = \"What can ChatGPT do?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>VectorDB建议：</b>\n",
    "    <li>没有极端高性能要求的，FAISS比较常用</li>\n",
    "    <li>Pinecone（付费-云服务）易用性比较好</li>\n",
    "    <li>有极端性能要求的，可以找专人优化ElasticSearch（或APU加速）</li>\n",
    "    <li>一些对比分析可参考：https://www.modb.pro/db/516016</li>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 向量检索：Retrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the key strengths of ChatGPT is its ability to generate text that is cont extually relevant to the input\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"What can ChatGPT do?\")\n",
    "\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不用向量检索会怎样？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ChatGPT ? \n",
      "Md. Sakibul Islam Sakib  \n",
      " \n",
      "ChatGPT is a conversational language model developed by OpenAI. It is part of the GPT (Generative\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import TFIDFRetriever  # 最传统的关键字加权检索\n",
    "\n",
    "retriever = TFIDFRetriever.from_documents(paragraphs)\n",
    "docs = retriever.get_relevant_documents(\"What can ChatGPT do?\")\n",
    "\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>为什么向量检索效果更好？\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、记忆封装：Memory\n",
    "\n",
    "### 3.1 对话上下文：ConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 你好啊\\nAI: 你也好啊'}\n",
      "{'history': 'Human: 你好啊\\nAI: 你也好啊\\nHuman: 你再好啊\\nAI: 你又好啊'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"你好啊\"}, {\"output\": \"你也好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"你再好啊\"}, {\"output\": \"你又好啊\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "只保留一个窗口的上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: 第二轮问\\nAI: 第二轮答\\nHuman: 第三轮问\\nAI: 第三轮答'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"第一轮问\"}, {\"output\": \"第一轮答\"})\n",
    "window.save_context({\"input\": \"第二轮问\"}, {\"output\": \"第二轮答\"})\n",
    "window.save_context({\"input\": \"第三轮问\"}, {\"output\": \"第三轮答\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 自动对历史信息做摘要：ConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': '\\n人类问AI助手你好，AI助手回答你好，表示自己是人类的AI助手，可以回答有关AGIClass的各种问题。'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    # buffer=\"The conversation is between a customer and a sales.\"\n",
    "    buffer=\"以中文表示\"\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"你好\"}, {\"output\": \"你好，我是你的AI助手。我能为你回答有关AGIClass的各种问题。\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、链架构：Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「Chains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.」\n",
    "\n",
    "- Chain 封装了一个既定的流程\n",
    "- 类比于函数封装了过程\n",
    "- 建造者模式（Builder Pattern）, 解耦各种复杂的组件\n",
    "\n",
    "### 4.1 一个最简单的 Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "智迅电子\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"为生产{product}的公司取一个亮眼中文名字：\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"电脑\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 在 Chain 中加入 Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是聊天机器人小瓜，你可以和人类聊天。\n",
      "\n",
      "以中文表示\n",
      "Human: 你是谁？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 你好，我是聊天机器人小瓜，很高兴认识你。\n",
      "---------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m你是聊天机器人小瓜，你可以和人类聊天。\n",
      "\n",
      "\n",
      "以中文表示\n",
      "人类问AI谁是，AI回答自己是聊天机器人小瓜，表示很高兴认识人类。\n",
      "Human: 我刚才问了你什么，你是怎么回答的？\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " 您刚才问我是谁，我回答说我是聊天机器人小瓜，表示很高兴认识您。\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"你是聊天机器人小瓜，你可以和人类聊天。\n",
    "\n",
    "{memory}\n",
    "Human: {human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"memory\", \"human_input\"], template=template\n",
    ")\n",
    "\n",
    "#memory = ConversationBufferMemory(memory_key=\"memory\")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(\n",
    "    temperature=0), buffer=\"以中文表示\", memory_key=\"memory\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "print(llm_chain.run(\"你是谁？\"))\n",
    "print(\"---------------\")\n",
    "output = llm_chain.run(\"我刚才问了你什么，你是怎么回答的？\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 一个复杂一点的 Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stuffdocchain.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ChatALL可以从https://github.com/sunner/ChatALL/releases 下载。'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"ChatALL.md\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(\n",
    "    temperature=0), chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "\n",
    "query = \"ChatALL在哪下载\"\n",
    "qa_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================qa_chain===============\n",
      "memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None combine_documents_chain=StuffDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='sk-fake-jupyterlabteach', openai_api_base='http://openai-proxy.default:8000/v1', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context', document_separator='\\n\\n') input_key='query' output_key='result' return_source_documents=False retriever=VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f2045737190>, search_type='similarity', search_kwargs={})\n",
      "======combine_documents_chain==========\n",
      "input_variables=['page_content'] output_parser=None partial_variables={} template='{page_content}' template_format='f-string' validate_template=True\n",
      "==============llm_chain================\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print('================qa_chain===============')\n",
    "print(qa_chain)\n",
    "print('======combine_documents_chain==========')\n",
    "print(qa_chain.combine_documents_chain.document_prompt)\n",
    "print('==============llm_chain================')\n",
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4 常用的基础 Chain 类型：Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m天降伞业\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m天降伞业:安全降落，信天由命\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "天降伞业:安全降落，信天由命\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9)\n",
    "name_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"为生产{product}的公司取一个亮眼中文名字：\",\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"为名为{name}的公司起一个Slogan，输出格式 name:slogan\",\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[name_chain, slogan_chain], verbose=True)\n",
    "\n",
    "print(overall_chain.run(\"雨伞\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 常用的基础 Chain 类型：Transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m我是警察，有事随时跟我联系，打我手机***********\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"job\": \"警察\"}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\"job\": \"警察\"}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# 例如：发给OpenAI之前，把用户隐私数据抹掉\n",
    "\n",
    "\n",
    "def anonymize(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    t = re.compile(\n",
    "        r'1(3\\d|4[4-9]|5[0-35-9]|6[67]|7[013-8]|8[0-9]|9[0-9])\\d{8}')\n",
    "    while True:\n",
    "        s = re.search(t, text)\n",
    "        if s:\n",
    "            text = text.replace(s.group(), '***********')\n",
    "        else:\n",
    "            break\n",
    "    return {\"output_text\": text}\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=anonymize\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"根据下述句子，提取候选人的职业:\\n{input}\\n输出JSON, 以job为key\",\n",
    ")\n",
    "\n",
    "task_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[transform_chain, task_chain], verbose=True)\n",
    "\n",
    "print(overall_chain.run(\"我是警察，有事随时跟我联系，打我手机13912345678\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 常用的基础 Chain 类型：Router\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "WindowsExpert: {'input': '帮我写个脚本，让Windows系统每天0点自动校准时间'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "答案：\n",
      "首先，您需要在Windows系统中打开控制台，在控制台输入以下命令：\n",
      "\n",
      "schtasks /create /tn \"Auto Sync Time\" /tr schtasks /run /sc daily /st 00:00:00\n",
      "\n",
      "接着，您可以运行这个任务，使Windows系统每天0点自动校准时间：\n",
      "\n",
      "schtasks /run /tn \"Auto Sync Time\"\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "LinuxExpert: {'input': '帮我写个cron脚本，让系统每天0点自动重启'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "答案:\n",
      "这是一个Linux Shell脚本，可以在crontab中使用：\n",
      "\n",
      "0 0 * * * /sbin/reboot\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "windows_template = \"\"\"\n",
    "你只会写DOS或Windows Shell脚本。你不会写任何其他语言的程序。你也不会写Linux脚本。\n",
    "\n",
    "用户问题:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "linux_template = \"\"\"\n",
    "你只会写Linux Shell脚本。你不会写任何其他语言的程序。你也不会写Windows脚本。\n",
    "\n",
    "用户问题:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"WindowsExpert\",\n",
    "        \"description\": \"擅长回答Windows Shell相关问题\",\n",
    "        \"prompt_template\": windows_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LinuxExpert\",\n",
    "        \"description\": \"擅长回答Linux Shell相关问题\",\n",
    "        \"prompt_template\": linux_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template,\n",
    "                            input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(chain.run(\"帮我写个脚本，让Windows系统每天0点自动校对时间\"))\n",
    "\n",
    "print(chain.run(\"帮我写个cron脚本，让系统每天0点自动重启\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>Router是否是一个必要的基础类型？\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 封装API调用：APIChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mhttps://api.open-meteo.com/v1/forecast?latitude=39.9042&longitude=116.4074&hourly=temperature_2m&temperature_unit=celsius\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"latitude\":39.875,\"longitude\":116.375,\"generationtime_ms\":0.11909008026123047,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":47.0,\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°C\"},\"hourly\":{\"time\":[\"2023-07-27T00:00\",\"2023-07-27T01:00\",\"2023-07-27T02:00\",\"2023-07-27T03:00\",\"2023-07-27T04:00\",\"2023-07-27T05:00\",\"2023-07-27T06:00\",\"2023-07-27T07:00\",\"2023-07-27T08:00\",\"2023-07-27T09:00\",\"2023-07-27T10:00\",\"2023-07-27T11:00\",\"2023-07-27T12:00\",\"2023-07-27T13:00\",\"2023-07-27T14:00\",\"2023-07-27T15:00\",\"2023-07-27T16:00\",\"2023-07-27T17:00\",\"2023-07-27T18:00\",\"2023-07-27T19:00\",\"2023-07-27T20:00\",\"2023-07-27T21:00\",\"2023-07-27T22:00\",\"2023-07-27T23:00\",\"2023-07-28T00:00\",\"2023-07-28T01:00\",\"2023-07-28T02:00\",\"2023-07-28T03:00\",\"2023-07-28T04:00\",\"2023-07-28T05:00\",\"2023-07-28T06:00\",\"2023-07-28T07:00\",\"2023-07-28T08:00\",\"2023-07-28T09:00\",\"2023-07-28T10:00\",\"2023-07-28T11:00\",\"2023-07-28T12:00\",\"2023-07-28T13:00\",\"2023-07-28T14:00\",\"2023-07-28T15:00\",\"2023-07-28T16:00\",\"2023-07-28T17:00\",\"2023-07-28T18:00\",\"2023-07-28T19:00\",\"2023-07-28T20:00\",\"2023-07-28T21:00\",\"2023-07-28T22:00\",\"2023-07-28T23:00\",\"2023-07-29T00:00\",\"2023-07-29T01:00\",\"2023-07-29T02:00\",\"2023-07-29T03:00\",\"2023-07-29T04:00\",\"2023-07-29T05:00\",\"2023-07-29T06:00\",\"2023-07-29T07:00\",\"2023-07-29T08:00\",\"2023-07-29T09:00\",\"2023-07-29T10:00\",\"2023-07-29T11:00\",\"2023-07-29T12:00\",\"2023-07-29T13:00\",\"2023-07-29T14:00\",\"2023-07-29T15:00\",\"2023-07-29T16:00\",\"2023-07-29T17:00\",\"2023-07-29T18:00\",\"2023-07-29T19:00\",\"2023-07-29T20:00\",\"2023-07-29T21:00\",\"2023-07-29T22:00\",\"2023-07-29T23:00\",\"2023-07-30T00:00\",\"2023-07-30T01:00\",\"2023-07-30T02:00\",\"2023-07-30T03:00\",\"2023-07-30T04:00\",\"2023-07-30T05:00\",\"2023-07-30T06:00\",\"2023-07-30T07:00\",\"2023-07-30T08:00\",\"2023-07-30T09:00\",\"2023-07-30T10:00\",\"2023-07-30T11:00\",\"2023-07-30T12:00\",\"2023-07-30T13:00\",\"2023-07-30T14:00\",\"2023-07-30T15:00\",\"2023-07-30T16:00\",\"2023-07-30T17:00\",\"2023-07-30T18:00\",\"2023-07-30T19:00\",\"2023-07-30T20:00\",\"2023-07-30T21:00\",\"2023-07-30T22:00\",\"2023-07-30T23:00\",\"2023-07-31T00:00\",\"2023-07-31T01:00\",\"2023-07-31T02:00\",\"2023-07-31T03:00\",\"2023-07-31T04:00\",\"2023-07-31T05:00\",\"2023-07-31T06:00\",\"2023-07-31T07:00\",\"2023-07-31T08:00\",\"2023-07-31T09:00\",\"2023-07-31T10:00\",\"2023-07-31T11:00\",\"2023-07-31T12:00\",\"2023-07-31T13:00\",\"2023-07-31T14:00\",\"2023-07-31T15:00\",\"2023-07-31T16:00\",\"2023-07-31T17:00\",\"2023-07-31T18:00\",\"2023-07-31T19:00\",\"2023-07-31T20:00\",\"2023-07-31T21:00\",\"2023-07-31T22:00\",\"2023-07-31T23:00\",\"2023-08-01T00:00\",\"2023-08-01T01:00\",\"2023-08-01T02:00\",\"2023-08-01T03:00\",\"2023-08-01T04:00\",\"2023-08-01T05:00\",\"2023-08-01T06:00\",\"2023-08-01T07:00\",\"2023-08-01T08:00\",\"2023-08-01T09:00\",\"2023-08-01T10:00\",\"2023-08-01T11:00\",\"2023-08-01T12:00\",\"2023-08-01T13:00\",\"2023-08-01T14:00\",\"2023-08-01T15:00\",\"2023-08-01T16:00\",\"2023-08-01T17:00\",\"2023-08-01T18:00\",\"2023-08-01T19:00\",\"2023-08-01T20:00\",\"2023-08-01T21:00\",\"2023-08-01T22:00\",\"2023-08-01T23:00\",\"2023-08-02T00:00\",\"2023-08-02T01:00\",\"2023-08-02T02:00\",\"2023-08-02T03:00\",\"2023-08-02T04:00\",\"2023-08-02T05:00\",\"2023-08-02T06:00\",\"2023-08-02T07:00\",\"2023-08-02T08:00\",\"2023-08-02T09:00\",\"2023-08-02T10:00\",\"2023-08-02T11:00\",\"2023-08-02T12:00\",\"2023-08-02T13:00\",\"2023-08-02T14:00\",\"2023-08-02T15:00\",\"2023-08-02T16:00\",\"2023-08-02T17:00\",\"2023-08-02T18:00\",\"2023-08-02T19:00\",\"2023-08-02T20:00\",\"2023-08-02T21:00\",\"2023-08-02T22:00\",\"2023-08-02T23:00\"],\"temperature_2m\":[27.6,28.8,30.7,32.4,33.9,35.0,34.6,34.6,34.3,34.0,33.3,32.6,31.5,27.8,27.6,27.4,26.9,26.5,26.2,26.1,26.1,25.9,25.4,25.3,25.7,26.1,26.9,27.0,26.1,26.0,27.3,28.1,28.4,28.3,28.0,27.8,27.3,26.9,26.5,26.3,26.0,25.7,25.5,25.4,25.3,25.2,25.1,25.2,25.4,26.2,26.8,26.7,27.1,27.7,28.2,28.2,27.9,27.1,26.2,25.7,25.4,25.1,24.9,24.8,24.7,24.6,24.4,24.4,24.2,24.1,24.1,24.2,24.2,24.4,24.5,24.5,24.6,24.8,23.7,23.8,24.0,23.9,23.8,23.8,24.0,24.0,23.8,23.7,23.7,23.6,23.6,23.5,23.3,23.2,23.2,23.1,23.2,23.3,23.4,23.6,23.8,24.1,24.4,24.7,25.1,25.4,25.6,25.7,25.7,25.6,25.5,25.3,25.2,25.1,25.1,25.0,24.9,24.9,25.1,25.3,25.5,25.8,26.2,26.4,26.6,26.6,26.6,28.1,28.3,28.3,28.0,27.4,26.8,26.5,26.3,26.0,25.8,25.5,25.3,25.1,25.0,25.1,25.4,25.9,26.7,27.9,29.3,30.6,31.8,32.8,33.6,34.1,34.3,34.1,33.1,31.7,30.4,29.7,29.1,28.6,28.1,27.6,27.2,26.9,26.7,26.7,27.1,27.6]}}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The temperature in Beijing today is 27.6°C.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
    "chain_new.run('北京今天气温，摄氏度')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 调用OpenAI Function Calling获得Pydantic输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a world class algorithm for extracting information in structured formats.\n",
      "Human: Use the given format to extract information from the following input:\n",
      "Human: 寄给亮马桥外交办公大楼的王卓然，13012345678\n",
      "Human: Tips: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Contact(name='王卓然', address='亮马桥外交办公大楼', tel='13012345678')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class Contact(BaseModel):\n",
    "    \"\"\"Extracting information about a contact persion.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    address: str = Field(..., description=\"The person's address\")\n",
    "    tel: str = Field(None, description=\"The person's telephone/mobile number\")\n",
    "\n",
    "prompt_msgs = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a world class algorithm for extracting information in structured formats.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Use the given format to extract information from the following input:\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate(messages=prompt_msgs)\n",
    "llm = ChatOpenAI(model=\"gpt-4-0613\", temperature=0)\n",
    "\n",
    "chain = create_structured_output_chain(Contact, llm, prompt, verbose=True)\n",
    "\n",
    "chain.run(\"寄给亮马桥外交办公大楼的王卓然，13012345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a world class algorithm for extracting information in structured formats.\n",
      "Human: Use the given format to extract information from the following input:\n",
      "Human: 寄给亮马桥外交办公大楼的王卓然，13012345678\n",
      "Human: Tips: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mname='王卓然' address='亮马桥外交办公大楼' tel='13012345678'\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mBEGIN:VCARD\n",
      "VERSION:2.1\n",
      "N:王卓然\n",
      "ADR:亮马桥外交办公大楼\n",
      "TEL:13012345678\n",
      "END:VCARD\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "BEGIN:VCARD\n",
      "VERSION:2.1\n",
      "N:王卓然\n",
      "ADR:亮马桥外交办公大楼\n",
      "TEL:13012345678\n",
      "END:VCARD\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from typing import Dict\n",
    "\n",
    "def process(inputs: Dict[str,Contact])->str:\n",
    "    person = inputs[\"contact\"]\n",
    "    return {\"text\":f\"BEGIN:VCARD\\nVERSION:2.1\\nN:{person.name}\\nADR:{person.address}\\nTEL:{person.tel}\\nEND:VCARD\"}\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"contact\"], output_variables=[\"text\"], transform=process\n",
    ")\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[chain, transform_chain], verbose=True)\n",
    "\n",
    "print(overall_chain.run(\"寄给亮马桥外交办公大楼的王卓然，13012345678\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 基于 Document 的 Chains\n",
    "\n",
    "<img src=\"stuff.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"refine.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"map_reduce.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"map_rerank.jpg\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapRerankDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "is June 19, 2023. Note that the paper needs to be fully reviewedby ARR in order to make a commitment, thus the latest date for ARR submission will be April15, 2023.\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "have to fill in the submission form in the Softconf/START system and upload an initialpdf of their papers before May 15, 2023 (23:59 GMT-11). The submission link is https://softconf.com/n/sigdial2023/ .Submission via ACL Rolling Review (ARR)Please refer to the ARR Call for Papers  for detailed information about submission guidelines toARR. The commitment deadline for authors to submit their reviewed papers, reviews, andmeta-review to SIGDIAL 2023 is June 19, 2023. Note that the paper needs to\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "whichare available as an Overleaf template  and also downloadable directly  (Latex and Word)Submissions must conform to the official ACL style guidelines, which are contained in thesetemplates. Submissions must be electronic, in PDF format.Submission DeadlineSIGDIAL will accept regular submissions through the Softconf/START system, as well ascommitment of already reviewed papers through the ACL Rolling Review (ARR) system.Regular submissionAuthors have to fill in the submission form in the\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "There will be 1 extra week given to the authors for updating the paper after the submission.Use the following link to commit the paper: https://forms.office.com/r/wxZHZ08iZmMentoringAcceptable submissions that require language (English) or organizational assistance will beflagged for mentoring, and accepted with a recommendation to revise with the help of amentor. An experienced mentor who has previously published in the SIGDIAL venue will thenhelp the authors of these flagged papers prepare\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The regular submission deadline is April 15, 2023. The ARR submission deadline is also April 15, 2023.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def set_verbose_recusively(chain):\n",
    "    chain.verbose = True\n",
    "    for attr in dir(chain):\n",
    "        if attr.endswith('_chain') and isinstance(getattr(chain,attr),Chain):\n",
    "            subchain=getattr(chain,attr)\n",
    "            set_verbose_recusively(subchain)\n",
    "\n",
    "loader = PyPDFLoader(\"SIGDIAL2023.pdf\")\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents(\n",
    "    [d.page_content for d in documents])\n",
    "# print(paragraphs)\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "db = FAISS.from_documents(paragraphs, embeddings)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"map_rerank\",\n",
    "    retriever=db.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "set_verbose_recusively(qa_chain)\n",
    "\n",
    "query = \"When is the regular submission deadline? When is the ARR submission deadline?\"\n",
    "qa_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、智能体架构：Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 什么是智能体（Agent）\n",
    "\n",
    "将大语言模型作为一个推理引擎。给定一个任务，智能体自动生成完成任务所需的步骤，执行相应动作（例如选择并调用工具），直到任务完成。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 先定义一些工具：Tools\n",
    "\n",
    "- 可以是一个函数或三方 API\n",
    "- 也可以把一个 Chain 或者 Agent 的 run()作为一个 Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool, tool\n",
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools([\"serpapi\"])\n",
    "tools += [weekday]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 智能体类型：ReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m我需要知道周杰伦的生日是哪一天，然后我可以使用weekday工具来找出那天是星期几。\n",
      "Action: Search\n",
      "Action Input: 周杰伦生日是哪一天\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mJanuary 18, 1979\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道周杰伦的生日是1月18日，1979年。我可以使用weekday工具来找出那天是星期几。\n",
      "Action: weekday\n",
      "Action Input: January 18, 1979\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThursday\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m我现在知道周杰伦的生日那天是星期四。\n",
      "Final Answer: 星期四\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'星期四'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"周杰伦生日那天是星期几\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 通过 OpenAI Function Calling 实现智能体\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `周杰伦的生日`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mJanuary 18, 1979\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `weekday` with `{'date_str': '1979-01-18'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThursday\u001b[0m\u001b[32;1m\u001b[1;3m周杰伦的生日（1979年1月18日）是星期四。\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'周杰伦的生日（1979年1月18日）是星期四。'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613', temperature=0)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method=\"generate\",\n",
    ")\n",
    "agent.run(\"周杰伦生日那天是星期几\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 智能体类型：SelfAskWithSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: Who is the wife of Feng Xiaogang?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mHe married actress Xu Fan in 1999. FilmographyEdit. As directorEdit. Year, English Title ...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: What movies has Xu Fan acted in?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mXu Fan is a Chinese actress and Asian Film Awards winner. She married film director Feng Xiaogang in 1999 and has starred in a number of films and television series directed by her husband.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSo the final answer is: Xu Fan has starred in a number of films and television series directed by her husband.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Xu Fan has starred in a number of films and television series directed by her husband.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\",\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(\n",
    "    tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True\n",
    ")\n",
    "self_ask_with_search.run(\n",
    "    \"冯小刚的老婆演过什么电影\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 智能体类型：Plan-and-Execute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PlanExec.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
      "steps=[Step(value='Access a reliable weather forecasting website or API to gather the weather data for Beijing and Shanghai for tomorrow.'), Step(value='Analyze the weather data for both cities, focusing on key aspects such as temperature, humidity, wind speed, and weather conditions (sunny, cloudy, rainy, etc.).'), Step(value='Compare the weather data of the two cities, highlighting the similarities and differences.'), Step(value='Write a report in Chinese, summarizing the weather forecast for both cities and the comparison between them.'), Step(value='Review the report to ensure it is accurate and clear.'), Step(value='Given the above steps taken, please respond to the users original question.\\n')]\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The assistant needs to access a reliable weather forecasting website or API to gather the weather data for Beijing and Shanghai for tomorrow. However, the assistant does not have the capability to directly access external APIs or websites. It can only provide information on how to do it.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I'm sorry for the misunderstanding, but I don't have the capability to directly access external APIs or websites. However, I can guide you on how to do it. You can use weather forecasting APIs like OpenWeatherMap, Weatherstack, or AccuWeather. These APIs allow you to access weather data for any location. You would need to sign up, get an API key, and make a GET request to their servers with the API key and the name of the city as parameters. The response will usually be in JSON format, which you can parse to get the weather data.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Access a reliable weather forecasting website or API to gather the weather data for Beijing and Shanghai for tomorrow.\n",
      "\n",
      "Response: I'm sorry for the misunderstanding, but I don't have the capability to directly access external APIs or websites. However, I can guide you on how to do it. You can use weather forecasting APIs like OpenWeatherMap, Weatherstack, or AccuWeather. These APIs allow you to access weather data for any location. You would need to sign up, get an API key, and make a GET request to their servers with the API key and the name of the city as parameters. The response will usually be in JSON format, which you can parse to get the weather data.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user wants to analyze the weather data for Beijing and Shanghai, focusing on key aspects such as temperature, humidity, wind speed, and weather conditions. Since I don't have the capability to directly access external APIs or websites, I can guide the user on how to analyze the data once they have it.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Once you have the weather data in JSON format, you can analyze it by extracting the key aspects you're interested in. Here's a general idea of how you might do it:\\n\\n1. Temperature: This is usually given in degrees Celsius or Fahrenheit. You might want to compare the temperatures of the two cities and see which one is hotter or colder.\\n\\n2. Humidity: This is given as a percentage. A higher percentage means the air is more humid. You can compare the humidity levels of the two cities.\\n\\n3. Wind Speed: This is usually given in meters per second or miles per hour. You can compare the wind speeds of the two cities to see which one is windier.\\n\\n4. Weather Conditions: This is usually a descriptive term like 'sunny', 'cloudy', 'rainy', etc. You can see what the weather conditions are like in each city.\\n\\nRemember, the exact way to extract this information will depend on the specific structure of the JSON data you receive from the weather API.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Analyze the weather data for both cities, focusing on key aspects such as temperature, humidity, wind speed, and weather conditions (sunny, cloudy, rainy, etc.).\n",
      "\n",
      "Response: Once you have the weather data in JSON format, you can analyze it by extracting the key aspects you're interested in. Here's a general idea of how you might do it:\n",
      "\n",
      "1. Temperature: This is usually given in degrees Celsius or Fahrenheit. You might want to compare the temperatures of the two cities and see which one is hotter or colder.\n",
      "\n",
      "2. Humidity: This is given as a percentage. A higher percentage means the air is more humid. You can compare the humidity levels of the two cities.\n",
      "\n",
      "3. Wind Speed: This is usually given in meters per second or miles per hour. You can compare the wind speeds of the two cities to see which one is windier.\n",
      "\n",
      "4. Weather Conditions: This is usually a descriptive term like 'sunny', 'cloudy', 'rainy', etc. You can see what the weather conditions are like in each city.\n",
      "\n",
      "Remember, the exact way to extract this information will depend on the specific structure of the JSON data you receive from the weather API.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user wants to compare the weather data of Beijing and Shanghai. Since I don't have the actual data, I can only provide a general approach on how to compare the data.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"To compare the weather data of Beijing and Shanghai, you can create a table or a chart with the key aspects you're interested in (temperature, humidity, wind speed, and weather conditions). For each aspect, note the values for both cities. Then, you can highlight the similarities and differences. For example, if both cities have similar temperatures but different humidity levels, that's a noteworthy point. Similarly, if both cities are sunny but one has a much higher wind speed, that's another interesting comparison. Remember, the goal is to draw meaningful insights from the data, so focus on the aspects that you find most revealing about the weather in these two cities.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Compare the weather data of the two cities, highlighting the similarities and differences.\n",
      "\n",
      "Response: To compare the weather data of Beijing and Shanghai, you can create a table or a chart with the key aspects you're interested in (temperature, humidity, wind speed, and weather conditions). For each aspect, note the values for both cities. Then, you can highlight the similarities and differences. For example, if both cities have similar temperatures but different humidity levels, that's a noteworthy point. Similarly, if both cities are sunny but one has a much higher wind speed, that's another interesting comparison. Remember, the goal is to draw meaningful insights from the data, so focus on the aspects that you find most revealing about the weather in these two cities.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user is asking for a report summarizing the weather forecast for Beijing and Shanghai in Chinese. However, I don't have the actual weather data for these cities. I can provide a template for how such a report could be structured in Chinese. \n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"以下是一个天气预报报告的模板：\\n\\n标题：北京和上海的天气预报及其比较\\n\\n一、北京的天气预报\\n明天，北京的天气将是______。预计最高温度为______度，最低温度为______度。湿度为______%，风速为______。\\n\\n二、上海的天气预报\\n明天，上海的天气将是______。预计最高温度为______度，最低温度为______度。湿度为______%，风速为______。\\n\\n三、北京和上海的天气比较\\n在温度方面，______。在湿度方面，______。在风速方面，______。总的来说，______。\\n\\n结论：______。\\n\\n请将上述空白处填入相应的天气数据。\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Write a report in Chinese, summarizing the weather forecast for both cities and the comparison between them.\n",
      "\n",
      "Response: 以下是一个天气预报报告的模板：\n",
      "\n",
      "标题：北京和上海的天气预报及其比较\n",
      "\n",
      "一、北京的天气预报\n",
      "明天，北京的天气将是______。预计最高温度为______度，最低温度为______度。湿度为______%，风速为______。\n",
      "\n",
      "二、上海的天气预报\n",
      "明天，上海的天气将是______。预计最高温度为______度，最低温度为______度。湿度为______%，风速为______。\n",
      "\n",
      "三、北京和上海的天气比较\n",
      "在温度方面，______。在湿度方面，______。在风速方面，______。总的来说，______。\n",
      "\n",
      "结论：______。\n",
      "\n",
      "请将上述空白处填入相应的天气数据。\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.agents import load_tools\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0)\n",
    "\n",
    "search = SerpAPIWrapper(params={\n",
    "    'engine': 'google', \n",
    "    'gl': 'cn', \n",
    "    'google_domain': 'google.com.hk', \n",
    "    'hl': 'zh-cn'\n",
    "})\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )\n",
    "]\n",
    "\n",
    "planner = load_chat_planner(llm)\n",
    "executor = load_agent_executor(llm, tools, verbose=True)\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)\n",
    "\n",
    "agent.run(\"分析北京明天天气，与上海明天天气对比，用中文写一遍报告\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、Callbacks\n",
    "\n",
    "回调函数，用于监测、记录调用过程中的信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCallbackHandler:\n",
    "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when Chat Model starts running.\"\"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Start: {'number': 1}\n",
      "On text: Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 1 = \u001b[0m\n",
      "Done!\n",
      "Chain Start: {'number': 2}\n",
      "On text: Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "Feed LLM with ['1 + 2 = ']\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class myhandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"Feed LLM with {prompts}\")\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"Chain Start: {inputs}\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        print(f\"Done!\")\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        print(f\"On text: {text}\")\n",
    "\n",
    "\n",
    "handler = myhandler()\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# 在构造的时候加回调，只触发这个对象对应的事件\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "x = chain.run(number=1)\n",
    "\n",
    "# 在运行的时候加回调，触发这个过程的所有子过程的事件\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "x = chain.run(number=2, callbacks=[handler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大模型时代软件的演变趋势\n",
    "\n",
    "<img src=\"agent.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>思考：</b>\n",
    "<ul>\n",
    "<li>从软件工程的角度，LangChain现阶段的缺点是什么</li>\n",
    "<li>距离智能体大规模应用，我们还有什么没解决的问题</li>\n",
    "<li>你觉得智能体有哪些可优化的方向</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangFlow\n",
    "\n",
    "<img src=\"https://github.com/logspace-ai/langflow/raw/main/img/langflow-demo.gif?raw=true\" style=\"margin-left: 0px\">\n",
    "\n",
    "https://github.com/logspace-ai/langflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "做个自己的 [ChatPDF(https://www.chatpdf.com/)](https://www.chatpdf.com/)（需科学访问）。需求：\n",
    "\n",
    "1. 从本地加载 PDF 文件，基于 PDF 的内容对话\n",
    "2. 可以无前端，只要能在命令行运行就行\n",
    "3. 其它随意发挥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 课后调查\n",
    "\n",
    "请点击链接或扫码填写问卷，帮助我们持续改进课程内容。谢谢！\n",
    "\n",
    "https://agiclass.feishu.cn/share/base/form/shrcnU6ywdMDS2caxf6gp7dYQ63\n",
    "\n",
    "<img src=\"../survey.png\" width=\"200\" style=\"margin-left: 0px\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
