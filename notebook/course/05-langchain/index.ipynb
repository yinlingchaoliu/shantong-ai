{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ LangChainï¼šä¸€å¥—åœ¨å¤§æ¨¡å‹èƒ½åŠ›ä¸Šå°è£…çš„å·¥å…·æ¡†æ¶\n",
    "2. å¦‚ä½•ç”¨å‡ è¡Œä»£ç å®ç°ä¸€ä¸ªå¤æ‚çš„ AI åº”ç”¨\n",
    "3. é¢å‘å¤§æ¨¡å‹çš„æµç¨‹å¼€å‘çš„è¿‡ç¨‹æŠ½è±¡\n",
    "\n",
    "å¼€å§‹ä¸Šè¯¾ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## å†™åœ¨å‰é¢\n",
    "\n",
    "- LangChain æ˜¯ä¸€å¥—é¢å‘å¤§æ¨¡å‹çš„å¼€å‘æ¡†æ¶\n",
    "- LangChain æ˜¯ AGI æ—¶ä»£è½¯ä»¶å·¥ç¨‹çš„ä¸€ä¸ªæ¢ç´¢å’ŒåŸå‹\n",
    "- LangChain å¹¶ä¸å®Œç¾ï¼Œè¿˜åœ¨ä¸æ–­è¿­ä»£ä¸­ï¼šæˆ‘å†™è¿™ä¸ªè¯¾ä»¶çš„æ—¶å€™æ˜¯ V0.0.200ï¼Œç°åœ¨æ˜¯ V0.0.241\n",
    "- å­¦ä¹  LangChain æ›´é‡è¦çš„æ˜¯å€Ÿé‰´å…¶æ€æƒ³ï¼Œå…·ä½“çš„æ¥å£å¯èƒ½å¾ˆå¿«å°±ä¼šæ”¹å˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## LangChain çš„æ ¸å¿ƒç»„ä»¶\n",
    "\n",
    "1. æ¨¡å‹ I/O å°è£…\n",
    "   - LLMsï¼šå¤§è¯­è¨€æ¨¡å‹\n",
    "   - Chat Modelsï¼šä¸€èˆ¬åŸºäº LLMsï¼Œä½†æŒ‰å¯¹è¯ç»“æ„é‡æ–°å°è£…\n",
    "   - PromptTempleï¼šæç¤ºè¯æ¨¡æ¿\n",
    "   - OutputParserï¼šè§£æè¾“å‡º\n",
    "2. æ•°æ®è¿æ¥å°è£…\n",
    "   - Document Loadersï¼šå„ç§æ ¼å¼æ–‡ä»¶çš„åŠ è½½å™¨\n",
    "   - Document transformersï¼šå¯¹æ–‡æ¡£çš„å¸¸ç”¨æ“ä½œï¼Œå¦‚ï¼šsplit, filter, translate, extract metadata, etc\n",
    "   - Text Embedding Modelsï¼šæ–‡æœ¬å‘é‡åŒ–è¡¨ç¤ºï¼Œç”¨äºæ£€ç´¢ç­‰æ“ä½œï¼ˆå•¥æ„æ€ï¼Ÿåˆ«æ€¥ï¼Œåé¢è¯¦ç»†è®²ï¼‰\n",
    "   - Verctor stores: ï¼ˆé¢å‘æ£€ç´¢çš„ï¼‰å‘é‡çš„å­˜å‚¨\n",
    "   - Retrievers: å‘é‡çš„æ£€ç´¢\n",
    "3. è®°å¿†å°è£…\n",
    "   - Memoryï¼šè¿™é‡Œä¸æ˜¯ç‰©ç†å†…å­˜ï¼Œä»æ–‡æœ¬çš„è§’åº¦ï¼Œå¯ä»¥ç†è§£ä¸ºâ€œä¸Šæ–‡â€ã€â€œå†å²è®°å½•â€æˆ–è€…è¯´â€œè®°å¿†åŠ›â€çš„ç®¡ç†\n",
    "4. æ¶æ„å°è£…\n",
    "   - Chainï¼šå®ç°ä¸€ä¸ªåŠŸèƒ½æˆ–è€…ä¸€ç³»åˆ—é¡ºåºåŠŸèƒ½ç»„åˆ\n",
    "   - Agentï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥ï¼Œè‡ªåŠ¨è§„åˆ’æ‰§è¡Œæ­¥éª¤ï¼Œè‡ªåŠ¨é€‰æ‹©æ¯æ­¥éœ€è¦çš„å·¥å…·ï¼Œæœ€ç»ˆå®Œæˆç”¨æˆ·æŒ‡å®šçš„åŠŸèƒ½\n",
    "     - Toolsï¼šè°ƒç”¨å¤–éƒ¨åŠŸèƒ½çš„å‡½æ•°ï¼Œä¾‹å¦‚ï¼šè°ƒ google æœç´¢ã€æ–‡ä»¶ I/Oã€Linux Shell ç­‰ç­‰\n",
    "     - Toolkitsï¼šæ“ä½œæŸè½¯ä»¶çš„ä¸€ç»„å·¥å…·é›†ï¼Œä¾‹å¦‚ï¼šæ“ä½œ DBã€æ“ä½œ Gmail ç­‰ç­‰\n",
    "5. Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€æ¨¡å‹ I/O å°è£…\n",
    "\n",
    "### 1.1 æ¨¡å‹ APIï¼šLLM vs. ChatModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain == 0.0.240rc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½¿ç”¨æˆ‘çš„äº§å“ï¼\\n\\nä½ å¯ä»¥åœ¨æˆ‘ä»¬çš„ç½‘ç«™ä¸Šæ‰¾åˆ°æœ‰å…³æˆ‘ä»¬äº§å“çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬äº§å“åŠŸèƒ½ã€ä»·æ ¼ã€è¯„è®ºç­‰ç­‰ã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸“ä¸šçš„å®¢æˆ·æœåŠ¡ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›æœ‰å…³äº§å“çš„å¸®åŠ©å’Œå’¨è¯¢ã€‚æˆ‘ä»¬ä¹Ÿæä¾›å…è´¹çš„æŠ€æœ¯æ”¯æŒæœåŠ¡ï¼Œå¦‚æœæ‚¨åœ¨ä½¿ç”¨æˆ‘ä»¬çš„äº§å“æ—¶é‡åˆ°ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥éšæ—¶è”'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = OpenAI()  # é»˜è®¤æ˜¯text-davinci-003æ¨¡å‹\n",
    "llm.predict(\"ä½ å¥½ï¼Œæ¬¢è¿\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä½ å¥½ï¼æ„Ÿè°¢æ‚¨çš„æ¬¢è¿ã€‚æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatOpenAI() #é»˜è®¤æ˜¯gpt-3.5-turbo\n",
    "chat_model.predict(\"ä½ å¥½ï¼Œæ¬¢è¿\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='æ¬¢è¿æ¥åˆ°AGIClassï¼è¯·é—®ä½ æ˜¯ç¬¬ä¸€æ¬¡æ¥ä¸Šè¯¾å—ï¼Ÿ', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯AGIClassçš„è¯¾ç¨‹åŠ©ç†ã€‚\"),\n",
    "    HumanMessage(content=\"æˆ‘æ¥ä¸Šè¯¾äº†\")\n",
    "]\n",
    "chat_model(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 æ¨¡å‹çš„è¾“å…¥ä¸è¾“å‡º\n",
    "\n",
    "<img src=\"model_io.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject']\n",
      "ç»™æˆ‘è®²ä¸ªå…³äºå°æ˜çš„ç¬‘è¯\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"ç»™æˆ‘è®²ä¸ªå…³äº{subject}çš„ç¬‘è¯\")\n",
    "print(template.input_variables)\n",
    "print(template.format(subject='å°æ˜'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>æŠŠPromptæ¨¡æ¿çœ‹ä½œå¸¦æœ‰å‚æ•°çš„å‡½æ•°ï¼Œä¸‹é¢çš„å†…å®¹å¯èƒ½æ›´å¥½ç†è§£\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "<b>ä½œä¸šï¼š</b>è‡ªå­¦ChatPromptTemplateçš„ä½¿ç”¨\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"command\": {\"title\": \"Command\", \"description\": \"linux shellå‘½ä»¤å\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"description\": \"å‘½ä»¤çš„å‚æ•° (name:value)\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}}, \"required\": [\"command\", \"arguments\"]}\n",
      "```\n",
      "====Prompt=====\n",
      "å°†ç”¨æˆ·çš„æŒ‡ä»¤è½¬æ¢æˆlinuxå‘½ä»¤.\n",
      "The output should be formatted as a JSON instance that conforms to the JSON schema below.\n",
      "\n",
      "As an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}\n",
      "the object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n",
      "\n",
      "Here is the output schema:\n",
      "```\n",
      "{\"properties\": {\"command\": {\"title\": \"Command\", \"description\": \"linux shellå‘½ä»¤å\", \"type\": \"string\"}, \"arguments\": {\"title\": \"Arguments\", \"description\": \"å‘½ä»¤çš„å‚æ•° (name:value)\", \"type\": \"object\", \"additionalProperties\": {\"type\": \"string\"}}}, \"required\": [\"command\", \"arguments\"]}\n",
      "```\n",
      "å°†ç³»ç»Ÿæ—¥æœŸè®¾ä¸º2023-04-01\n",
      "====Output=====\n",
      "{\n",
      "  \"command\": \"date\",\n",
      "  \"arguments\": {\n",
      "    \"-s\": \"2023-04-01\"\n",
      "  }\n",
      "}\n",
      "====Parsed=====\n",
      "command='date' arguments={'-s': '2023-04-01'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "\n",
    "def chinese_friendly(string):\n",
    "    lines = string.split('\\n')\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.startswith('{') and line.endswith('}'):\n",
    "            try:\n",
    "                lines[i] = json.dumps(json.loads(line), ensure_ascii=False)\n",
    "            except:\n",
    "                pass\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "model_name = 'gpt-4'\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# å®šä¹‰ä½ çš„è¾“å‡ºæ ¼å¼\n",
    "\n",
    "\n",
    "class Command(BaseModel):\n",
    "    command: str = Field(description=\"linux shellå‘½ä»¤å\")\n",
    "    arguments: Dict[str, str] = Field(description=\"å‘½ä»¤çš„å‚æ•° (name:value)\")\n",
    "\n",
    "    # ä½ å¯ä»¥æ·»åŠ è‡ªå®šä¹‰çš„æ ¡éªŒæœºåˆ¶\n",
    "    @validator('command')\n",
    "    def no_space(cls, field):\n",
    "        if \" \" in field or \"\\t\" in field or \"\\n\" in field:\n",
    "            raise ValueError(\"å‘½ä»¤åä¸­ä¸èƒ½åŒ…å«ç©ºæ ¼æˆ–å›è½¦!\")\n",
    "        return field\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Command)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"å°†ç”¨æˆ·çš„æŒ‡ä»¤è½¬æ¢æˆlinuxå‘½ä»¤.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print(chinese_friendly(parser.get_format_instructions()))\n",
    "\n",
    "\n",
    "query = \"å°†ç³»ç»Ÿæ—¥æœŸè®¾ä¸º2023-04-01\"\n",
    "model_input = prompt.format_prompt(query=query)\n",
    "\n",
    "#print(parser.get_format_instructions())\n",
    "\n",
    "print(\"====Prompt=====\")\n",
    "print(chinese_friendly(model_input.to_string()))\n",
    "\n",
    "output = model(model_input.to_string())\n",
    "print(\"====Output=====\")\n",
    "print(output)\n",
    "print(\"====Parsed=====\")\n",
    "cmd = parser.parse(output)\n",
    "print(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€æ•°æ®è¿æ¥å°è£…\n",
    "\n",
    "<img src=\"data_connection.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 2.1 æ–‡æ¡£åŠ è½½å™¨ï¼šDocument Loaders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ChatGPT ? \n",
      "Md. Sakibul Islam Sakib  \n",
      " \n",
      "ChatGPT is a conversational language model developed by OpenAI. It is part of the GPT (Generative \n",
      "Pretrained  Transformer) family of models, which are based on the Transformer architecture and trained on \n",
      "vast amounts of text data to generate human -like text.  \n",
      "ChatGPT is designed to generate text in response to an input prompt, making it well suited for \n",
      "conversatio nal applications such as chatbots, customer service agents, and virtual assistants. The model has \n",
      "been trained on a diverse range of conversational data, including websites, books, and social media, \n",
      "allowing it to generate text that is coherent, contextual ly relevant, and often similar to text produced by \n",
      "humans.  \n",
      "To use ChatGPT, a user provides an input prompt, such as a question or statement, which is then fed into \n",
      "the model. The model then generates a response based on its understanding of the input and i ts training \n",
      "data. The model can generate multiple responses for a single input, and the responses can vary in length, \n",
      "style, and content depending on the context of the input.  \n",
      "One of the key strengths of ChatGPT is its ability to generate text that is cont extually relevant to the input \n",
      "prompt. For example, if a user asks a question about the weather, ChatGPT can generate a response that \n",
      "includes relevant information about the weather, such as temperature, precipitation, and wind conditions. \n",
      "If the user then  asks a follow -up question, ChatGPT can use the previous conversation as context to generate \n",
      "a response that is relevant to the previous conversation.  \n",
      "ChatGPT can also generate text that is coherent and flows well, allowing it to participate in longer \n",
      "conv ersations. The model has been trained on a large amount of conversational data, and has learned how \n",
      "to generate text that is grammatically correct, uses appropriate vocabulary and tone, and is coherent with \n",
      "the input prompt and the overall conversation.  \n",
      "In addition to generating text, ChatGPT can also be used for other NLP tasks such as question answering, \n",
      "summarization, and text classification. The model has been fine -tuned on specific tasks, allowing it to \n",
      "perform well on these tasks while still retaining  its ability to generate human -like text.  \n",
      "ChatGPT is part of a larger trend of using large language models for conversational applications. These \n",
      "models have the potential to revolutionize the way we interact with technology, allowing us to interact with \n",
      "devices and services in a more natural and intuitive way. However, it is important to note that these models \n",
      "are not perfect, and may sometimes generate text that is irrelevant, offensive, or factually incorrect.  \n",
      " \n",
      "In conclusion, ChatGPT is a powerful conver sational language model that is capable of generating human -\n",
      "like text in response to an input prompt. It has a wide range of potential applications, from chatbots and \n",
      "virtual assistants to NLP tasks such as question answering and text classification. Howev er, it is important \n",
      "to carefully consider the limitations and potential risks associated with these models, and to use them \n",
      "responsibly.  \n",
      "View publication stats\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"WhatisChatGPT.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "print(pages[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 æ–‡æ¡£å¤„ç†å™¨\n",
    "\n",
    "ä¾‹ 1ï¼šTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ChatGPT ? \n",
      "Md. Sakibul Islam Sakib\n",
      "-------\n",
      "ChatGPT is a conversational language model\n",
      "-------\n",
      "model developed by OpenAI. It is part of the GPT\n",
      "-------\n",
      "the GPT (Generative\n",
      "-------\n",
      "Pretrained  Transformer) family of models, which\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,  # æ€è€ƒï¼šä¸ºä»€ä¹ˆè¦åšoverlap\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents([pages[0].page_content])\n",
    "for para in paragraphs[:5]:\n",
    "    print(para.page_content)\n",
    "    print('-------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¾‹ 2ï¼šDoctran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install doctran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatGPTæ˜¯ç”±OpenAIå¼€å‘çš„ä¸€ç§å¯¹è¯è¯­è¨€æ¨¡å‹ã€‚å®ƒæ˜¯GPTï¼ˆç”Ÿæˆé¢„è®­ç»ƒå˜å‹å™¨ï¼‰æ¨¡å‹ç³»åˆ—çš„ä¸€éƒ¨åˆ†ï¼ŒåŸºäºå˜å‹å™¨æ¶æ„ï¼Œå¹¶åœ¨å¤§é‡æ–‡æœ¬æ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä»¥ç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚ChatGPTæ—¨åœ¨æ ¹æ®è¾“å…¥æç¤ºç”Ÿæˆæ–‡æœ¬ï¼Œéå¸¸é€‚ç”¨äºèŠå¤©æœºå™¨äººã€å®¢æœä»£ç†å’Œè™šæ‹ŸåŠ©æ‰‹ç­‰å¯¹è¯åº”ç”¨ã€‚è¯¥æ¨¡å‹ç»è¿‡å¤šæ ·åŒ–çš„å¯¹è¯æ•°æ®è®­ç»ƒï¼ŒåŒ…æ‹¬ç½‘ç«™ã€ä¹¦ç±å’Œç¤¾äº¤åª’ä½“ï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆè¿è´¯ã€ä¸Šä¸‹æ–‡ç›¸å…³ä¸”é€šå¸¸ç±»ä¼¼äºäººç±»ç”Ÿæˆçš„æ–‡æœ¬ã€‚ç”¨æˆ·æä¾›ä¸€ä¸ªè¾“å…¥æç¤ºï¼Œä¾‹å¦‚é—®é¢˜æˆ–é™ˆè¿°ï¼Œç„¶åå°†å…¶è¾“å…¥æ¨¡å‹ã€‚æ¨¡å‹æ ¹æ®å¯¹è¾“å…¥çš„ç†è§£å’Œè®­ç»ƒæ•°æ®ç”Ÿæˆå“åº”ã€‚æ¨¡å‹å¯ä»¥ä¸ºå•ä¸ªè¾“å…¥ç”Ÿæˆå¤šä¸ªå“åº”ï¼Œå“åº”çš„é•¿åº¦ã€é£æ ¼å’Œå†…å®¹å¯ä»¥æ ¹æ®è¾“å…¥çš„ä¸Šä¸‹æ–‡è€Œå˜åŒ–ã€‚ChatGPTçš„ä¸€ä¸ªå…³é”®ä¼˜åŠ¿æ˜¯å…¶èƒ½å¤Ÿç”Ÿæˆä¸è¾“å…¥æç¤ºç›¸å…³çš„æ–‡æœ¬ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨æˆ·è¯¢é—®å¤©æ°”é—®é¢˜ï¼ŒChatGPTå¯ä»¥ç”ŸæˆåŒ…å«ä¸å¤©æ°”ç›¸å…³çš„ä¿¡æ¯ï¼ˆå¦‚æ¸©åº¦ã€é™æ°´å’Œé£å†µï¼‰çš„å“åº”ã€‚å¦‚æœç”¨æˆ·éšåæå‡ºè·Ÿè¿›é—®é¢˜ï¼ŒChatGPTå¯ä»¥ä½¿ç”¨å…ˆå‰çš„å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œç”Ÿæˆä¸å…ˆå‰å¯¹è¯ç›¸å…³çš„å“åº”ã€‚ChatGPTè¿˜å¯ä»¥ç”Ÿæˆè¿è´¯æµç•…çš„æ–‡æœ¬ï¼Œä½¿å…¶èƒ½å¤Ÿå‚ä¸æ›´é•¿çš„å¯¹è¯ã€‚è¯¥æ¨¡å‹ç»è¿‡å¤§é‡å¯¹è¯æ•°æ®çš„è®­ç»ƒï¼Œå·²ç»å­¦ä¼šäº†å¦‚ä½•ç”Ÿæˆè¯­æ³•æ­£ç¡®ã€ä½¿ç”¨é€‚å½“çš„è¯æ±‡å’Œè¯­æ°”ï¼Œå¹¶ä¸è¾“å…¥æç¤ºå’Œæ•´ä½“å¯¹è¯ä¸€è‡´çš„æ–‡æœ¬ã€‚é™¤äº†ç”Ÿæˆæ–‡æœ¬ï¼ŒChatGPTè¿˜å¯ä»¥ç”¨äºå…¶ä»–è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ï¼Œå¦‚é—®ç­”ã€æ‘˜è¦å’Œæ–‡æœ¬åˆ†ç±»ã€‚è¯¥æ¨¡å‹ç»è¿‡ç‰¹å®šä»»åŠ¡çš„å¾®è°ƒï¼Œä½¿å…¶åœ¨è¿™äº›ä»»åŠ¡ä¸Šè¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶ä»ä¿ç•™ç”Ÿæˆç±»ä¼¼äººç±»æ–‡æœ¬çš„èƒ½åŠ›ã€‚ChatGPTæ˜¯ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹è¯åº”ç”¨çš„ä¸€ä¸ªé‡è¦è¶‹åŠ¿çš„ä¸€éƒ¨åˆ†ã€‚è¿™äº›æ¨¡å‹æœ‰æ½œåŠ›å½»åº•æ”¹å˜æˆ‘ä»¬ä¸æŠ€æœ¯äº’åŠ¨çš„æ–¹å¼ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥æ›´è‡ªç„¶ã€æ›´ç›´è§‚çš„æ–¹å¼ä¸è®¾å¤‡å’ŒæœåŠ¡è¿›è¡Œäº¤äº’ã€‚ç„¶è€Œï¼Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹å¹¶ä¸å®Œç¾ï¼Œæœ‰æ—¶å¯èƒ½ç”Ÿæˆä¸ä¸»é¢˜æ— å…³ã€å†’çŠ¯æ€§æˆ–äº‹å®é”™è¯¯çš„æ–‡æœ¬ã€‚æ€»ä¹‹ï¼ŒChatGPTæ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥æç¤ºç”Ÿæˆç±»ä¼¼äººç±»çš„æ–‡æœ¬ã€‚å®ƒå…·æœ‰å¹¿æ³›çš„æ½œåœ¨åº”ç”¨ï¼Œä»èŠå¤©æœºå™¨äººå’Œè™šæ‹ŸåŠ©æ‰‹åˆ°é—®ç­”å’Œæ–‡æœ¬åˆ†ç±»ç­‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ã€‚ç„¶è€Œï¼Œéœ€è¦ä»”ç»†è€ƒè™‘è¿™äº›æ¨¡å‹çš„é™åˆ¶å’Œæ½œåœ¨é£é™©ï¼Œå¹¶è´Ÿè´£ä»»åœ°ä½¿ç”¨å®ƒä»¬ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_transformers import DoctranTextTranslator\n",
    "\n",
    "translator = DoctranTextTranslator(\n",
    "    openai_api_model=\"gpt-3.5-turbo\", language=\"chinese\")\n",
    "\n",
    "translated_document = await translator.atransform_documents(pages)\n",
    "\n",
    "print(translated_document[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 æ–‡æ¡£å‘é‡åŒ–ï¼šText Embeddings\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>Embeddingï¼š</b>å°†ç›®æ ‡ç‰©ä½“ï¼ˆè¯ã€å¥å­ã€æ–‡ç« ï¼‰è¡¨ç¤ºæˆå‘é‡çš„æ–¹æ³•\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[-0.011436891742050648, -0.012987656518816948, 0.00902028288692236, -0.01197319757193327, -0.02477347105741501, 0.01448672916740179, -0.021891633048653603, -0.005188601091504097, -0.0012567657977342606, -0.03370329365134239]\n",
      "1536\n",
      "[-0.0026156024105142475, 0.0007669371424757893, -0.0046187359068776655, -0.005695960389375307, -0.010613725343519495, 0.026026324865937287, -0.014497498739680675, -0.001994126675894642, -0.014295743539741444, -0.018316421352086065]\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "text = \"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•\"\n",
    "document = \"æµ‹è¯•æ–‡æ¡£\"\n",
    "query_vec = embeddings.embed_query(text)\n",
    "doc_vec = embeddings.embed_documents([document])\n",
    "\n",
    "print(len(query_vec))\n",
    "print(query_vec[:10])  # ä¸ºäº†å±•ç¤ºæ–¹ä¾¿ï¼Œåªæ‰“å°å‰10ç»´\n",
    "print(len(doc_vec[0]))\n",
    "print(doc_vec[0][:10])  # ä¸ºäº†å±•ç¤ºæ–¹ä¾¿ï¼Œåªæ‰“å°å‰10ç»´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 å‘é‡çš„å­˜å‚¨ï¼ˆä¸ç´¢å¼•ï¼‰ï¼šVectorstores\n",
    "\n",
    "<img src=\"vector_stores.jpg\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the key strengths of ChatGPT is its ability to generate text that is cont extually relevant to the input\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(paragraphs, embeddings)\n",
    "\n",
    "query = \"What can ChatGPT do?\"\n",
    "docs = db.similarity_search(query)\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<b>VectorDBå»ºè®®ï¼š</b>\n",
    "    <li>æ²¡æœ‰æç«¯é«˜æ€§èƒ½è¦æ±‚çš„ï¼ŒFAISSæ¯”è¾ƒå¸¸ç”¨</li>\n",
    "    <li>Pineconeï¼ˆä»˜è´¹-äº‘æœåŠ¡ï¼‰æ˜“ç”¨æ€§æ¯”è¾ƒå¥½</li>\n",
    "    <li>æœ‰æç«¯æ€§èƒ½è¦æ±‚çš„ï¼Œå¯ä»¥æ‰¾ä¸“äººä¼˜åŒ–ElasticSearchï¼ˆæˆ–APUåŠ é€Ÿï¼‰</li>\n",
    "    <li>ä¸€äº›å¯¹æ¯”åˆ†æå¯å‚è€ƒï¼šhttps://www.modb.pro/db/516016</li>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 å‘é‡æ£€ç´¢ï¼šRetrievers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One of the key strengths of ChatGPT is its ability to generate text that is cont extually relevant to the input\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"What can ChatGPT do?\")\n",
    "\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä¸ç”¨å‘é‡æ£€ç´¢ä¼šæ€æ ·ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is ChatGPT ? \n",
      "Md. Sakibul Islam Sakib  \n",
      " \n",
      "ChatGPT is a conversational language model developed by OpenAI. It is part of the GPT (Generative\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import TFIDFRetriever  # æœ€ä¼ ç»Ÿçš„å…³é”®å­—åŠ æƒæ£€ç´¢\n",
    "\n",
    "retriever = TFIDFRetriever.from_documents(paragraphs)\n",
    "docs = retriever.get_relevant_documents(\"What can ChatGPT do?\")\n",
    "\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>ä¸ºä»€ä¹ˆå‘é‡æ£€ç´¢æ•ˆæœæ›´å¥½ï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€è®°å¿†å°è£…ï¼šMemory\n",
    "\n",
    "### 3.1 å¯¹è¯ä¸Šä¸‹æ–‡ï¼šConversationBufferMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š'}\n",
      "{'history': 'Human: ä½ å¥½å•Š\\nAI: ä½ ä¹Ÿå¥½å•Š\\nHuman: ä½ å†å¥½å•Š\\nAI: ä½ åˆå¥½å•Š'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "\n",
    "history = ConversationBufferMemory()\n",
    "history.save_context({\"input\": \"ä½ å¥½å•Š\"}, {\"output\": \"ä½ ä¹Ÿå¥½å•Š\"})\n",
    "\n",
    "print(history.load_memory_variables({}))\n",
    "\n",
    "history.save_context({\"input\": \"ä½ å†å¥½å•Š\"}, {\"output\": \"ä½ åˆå¥½å•Š\"})\n",
    "\n",
    "print(history.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åªä¿ç•™ä¸€ä¸ªçª—å£çš„ä¸Šä¸‹æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': 'Human: ç¬¬äºŒè½®é—®\\nAI: ç¬¬äºŒè½®ç­”\\nHuman: ç¬¬ä¸‰è½®é—®\\nAI: ç¬¬ä¸‰è½®ç­”'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "window = ConversationBufferWindowMemory(k=2)\n",
    "window.save_context({\"input\": \"ç¬¬ä¸€è½®é—®\"}, {\"output\": \"ç¬¬ä¸€è½®ç­”\"})\n",
    "window.save_context({\"input\": \"ç¬¬äºŒè½®é—®\"}, {\"output\": \"ç¬¬äºŒè½®ç­”\"})\n",
    "window.save_context({\"input\": \"ç¬¬ä¸‰è½®é—®\"}, {\"output\": \"ç¬¬ä¸‰è½®ç­”\"})\n",
    "print(window.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 è‡ªåŠ¨å¯¹å†å²ä¿¡æ¯åšæ‘˜è¦ï¼šConversationSummaryMemory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'history': '\\näººç±»é—®AIåŠ©æ‰‹ä½ å¥½ï¼ŒAIåŠ©æ‰‹å›ç­”ä½ å¥½ï¼Œè¡¨ç¤ºè‡ªå·±æ˜¯äººç±»çš„AIåŠ©æ‰‹ï¼Œå¯ä»¥å›ç­”æœ‰å…³AGIClassçš„å„ç§é—®é¢˜ã€‚'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=OpenAI(temperature=0),\n",
    "    # buffer=\"The conversation is between a customer and a sales.\"\n",
    "    buffer=\"ä»¥ä¸­æ–‡è¡¨ç¤º\"\n",
    ")\n",
    "memory.save_context(\n",
    "    {\"input\": \"ä½ å¥½\"}, {\"output\": \"ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„AIåŠ©æ‰‹ã€‚æˆ‘èƒ½ä¸ºä½ å›ç­”æœ‰å…³AGIClassçš„å„ç§é—®é¢˜ã€‚\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å››ã€é“¾æ¶æ„ï¼šChain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ã€ŒChains allow us to combine multiple components together to create a single, coherent application. For example, we can create a chain that takes user input, formats it with a PromptTemplate, and then passes the formatted response to an LLM. We can build more complex chains by combining multiple chains together, or by combining chains with other components.ã€\n",
    "\n",
    "- Chain å°è£…äº†ä¸€ä¸ªæ—¢å®šçš„æµç¨‹\n",
    "- ç±»æ¯”äºå‡½æ•°å°è£…äº†è¿‡ç¨‹\n",
    "- å»ºé€ è€…æ¨¡å¼ï¼ˆBuilder Patternï¼‰, è§£è€¦å„ç§å¤æ‚çš„ç»„ä»¶\n",
    "\n",
    "### 4.1 ä¸€ä¸ªæœ€ç®€å•çš„ Chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ™ºè¿…ç”µå­\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"ä¸ºç”Ÿäº§{product}çš„å…¬å¸å–ä¸€ä¸ªäº®çœ¼ä¸­æ–‡åå­—ï¼š\",\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "print(chain.run(\"ç”µè„‘\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 åœ¨ Chain ä¸­åŠ å…¥ Memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mä½ æ˜¯èŠå¤©æœºå™¨äººå°ç“œï¼Œä½ å¯ä»¥å’Œäººç±»èŠå¤©ã€‚\n",
      "\n",
      "ä»¥ä¸­æ–‡è¡¨ç¤º\n",
      "Human: ä½ æ˜¯è°ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " ä½ å¥½ï¼Œæˆ‘æ˜¯èŠå¤©æœºå™¨äººå°ç“œï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚\n",
      "---------------\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mä½ æ˜¯èŠå¤©æœºå™¨äººå°ç“œï¼Œä½ å¯ä»¥å’Œäººç±»èŠå¤©ã€‚\n",
      "\n",
      "\n",
      "ä»¥ä¸­æ–‡è¡¨ç¤º\n",
      "äººç±»é—®AIè°æ˜¯ï¼ŒAIå›ç­”è‡ªå·±æ˜¯èŠå¤©æœºå™¨äººå°ç“œï¼Œè¡¨ç¤ºå¾ˆé«˜å…´è®¤è¯†äººç±»ã€‚\n",
      "Human: æˆ‘åˆšæ‰é—®äº†ä½ ä»€ä¹ˆï¼Œä½ æ˜¯æ€ä¹ˆå›ç­”çš„ï¼Ÿ\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " æ‚¨åˆšæ‰é—®æˆ‘æ˜¯è°ï¼Œæˆ‘å›ç­”è¯´æˆ‘æ˜¯èŠå¤©æœºå™¨äººå°ç“œï¼Œè¡¨ç¤ºå¾ˆé«˜å…´è®¤è¯†æ‚¨ã€‚\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"ä½ æ˜¯èŠå¤©æœºå™¨äººå°ç“œï¼Œä½ å¯ä»¥å’Œäººç±»èŠå¤©ã€‚\n",
    "\n",
    "{memory}\n",
    "Human: {human_input}\n",
    "AI:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"memory\", \"human_input\"], template=template\n",
    ")\n",
    "\n",
    "#memory = ConversationBufferMemory(memory_key=\"memory\")\n",
    "\n",
    "memory = ConversationSummaryMemory(llm=OpenAI(\n",
    "    temperature=0), buffer=\"ä»¥ä¸­æ–‡è¡¨ç¤º\", memory_key=\"memory\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OpenAI(),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "print(llm_chain.run(\"ä½ æ˜¯è°ï¼Ÿ\"))\n",
    "print(\"---------------\")\n",
    "output = llm_chain.run(\"æˆ‘åˆšæ‰é—®äº†ä½ ä»€ä¹ˆï¼Œä½ æ˜¯æ€ä¹ˆå›ç­”çš„ï¼Ÿ\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ä¸€ä¸ªå¤æ‚ä¸€ç‚¹çš„ Chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"stuffdocchain.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ChatALLå¯ä»¥ä»https://github.com/sunner/ChatALL/releases ä¸‹è½½ã€‚'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "loader = UnstructuredMarkdownLoader(\"ChatALL.md\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(\n",
    "    temperature=0), chain_type=\"stuff\", retriever=db.as_retriever())\n",
    "\n",
    "query = \"ChatALLåœ¨å“ªä¸‹è½½\"\n",
    "qa_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================qa_chain===============\n",
      "memory=None callbacks=None callback_manager=None verbose=False tags=None metadata=None combine_documents_chain=StuffDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\", template_format='f-string', validate_template=True), llm=OpenAI(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<class 'openai.api_resources.completion.Completion'>, model_name='text-davinci-003', temperature=0.0, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0, n=1, best_of=1, model_kwargs={}, openai_api_key='sk-fake-jupyterlabteach', openai_api_base='http://openai-proxy.default:8000/v1', openai_organization='', openai_proxy='', batch_size=20, request_timeout=None, logit_bias={}, max_retries=6, streaming=False, allowed_special=set(), disallowed_special='all', tiktoken_model_name=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context', document_separator='\\n\\n') input_key='query' output_key='result' return_source_documents=False retriever=VectorStoreRetriever(tags=['FAISS'], metadata=None, vectorstore=<langchain.vectorstores.faiss.FAISS object at 0x7f2045737190>, search_type='similarity', search_kwargs={})\n",
      "======combine_documents_chain==========\n",
      "input_variables=['page_content'] output_parser=None partial_variables={} template='{page_content}' template_format='f-string' validate_template=True\n",
      "==============llm_chain================\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "{context}\n",
      "\n",
      "Question: {question}\n",
      "Helpful Answer:\n"
     ]
    }
   ],
   "source": [
    "print('================qa_chain===============')\n",
    "print(qa_chain)\n",
    "print('======combine_documents_chain==========')\n",
    "print(qa_chain.combine_documents_chain.document_prompt)\n",
    "print('==============llm_chain================')\n",
    "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4 å¸¸ç”¨çš„åŸºç¡€ Chain ç±»å‹ï¼šSequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3må¤©é™ä¼ä¸š\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3må¤©é™ä¼ä¸š:å®‰å…¨é™è½ï¼Œä¿¡å¤©ç”±å‘½\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "å¤©é™ä¼ä¸š:å®‰å…¨é™è½ï¼Œä¿¡å¤©ç”±å‘½\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9)\n",
    "name_prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"ä¸ºç”Ÿäº§{product}çš„å…¬å¸å–ä¸€ä¸ªäº®çœ¼ä¸­æ–‡åå­—ï¼š\",\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt=name_prompt)\n",
    "\n",
    "slogan_prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"],\n",
    "    template=\"ä¸ºåä¸º{name}çš„å…¬å¸èµ·ä¸€ä¸ªSloganï¼Œè¾“å‡ºæ ¼å¼ name:slogan\",\n",
    ")\n",
    "\n",
    "slogan_chain = LLMChain(llm=llm, prompt=slogan_prompt)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[name_chain, slogan_chain], verbose=True)\n",
    "\n",
    "print(overall_chain.run(\"é›¨ä¼\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 å¸¸ç”¨çš„åŸºç¡€ Chain ç±»å‹ï¼šTransform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mæˆ‘æ˜¯è­¦å¯Ÿï¼Œæœ‰äº‹éšæ—¶è·Ÿæˆ‘è”ç³»ï¼Œæ‰“æˆ‘æ‰‹æœº***********\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"job\": \"è­¦å¯Ÿ\"}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{\"job\": \"è­¦å¯Ÿ\"}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# ä¾‹å¦‚ï¼šå‘ç»™OpenAIä¹‹å‰ï¼ŒæŠŠç”¨æˆ·éšç§æ•°æ®æŠ¹æ‰\n",
    "\n",
    "\n",
    "def anonymize(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    t = re.compile(\n",
    "        r'1(3\\d|4[4-9]|5[0-35-9]|6[67]|7[013-8]|8[0-9]|9[0-9])\\d{8}')\n",
    "    while True:\n",
    "        s = re.search(t, text)\n",
    "        if s:\n",
    "            text = text.replace(s.group(), '***********')\n",
    "        else:\n",
    "            break\n",
    "    return {\"output_text\": text}\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"text\"], output_variables=[\"output_text\"], transform=anonymize\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=0.9)\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    template=\"æ ¹æ®ä¸‹è¿°å¥å­ï¼Œæå–å€™é€‰äººçš„èŒä¸š:\\n{input}\\nè¾“å‡ºJSON, ä»¥jobä¸ºkey\",\n",
    ")\n",
    "\n",
    "task_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[transform_chain, task_chain], verbose=True)\n",
    "\n",
    "print(overall_chain.run(\"æˆ‘æ˜¯è­¦å¯Ÿï¼Œæœ‰äº‹éšæ—¶è·Ÿæˆ‘è”ç³»ï¼Œæ‰“æˆ‘æ‰‹æœº13912345678\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 å¸¸ç”¨çš„åŸºç¡€ Chain ç±»å‹ï¼šRouter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "WindowsExpert: {'input': 'å¸®æˆ‘å†™ä¸ªè„šæœ¬ï¼Œè®©Windowsç³»ç»Ÿæ¯å¤©0ç‚¹è‡ªåŠ¨æ ¡å‡†æ—¶é—´'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ç­”æ¡ˆï¼š\n",
      "é¦–å…ˆï¼Œæ‚¨éœ€è¦åœ¨Windowsç³»ç»Ÿä¸­æ‰“å¼€æ§åˆ¶å°ï¼Œåœ¨æ§åˆ¶å°è¾“å…¥ä»¥ä¸‹å‘½ä»¤ï¼š\n",
      "\n",
      "schtasks /create /tn \"Auto Sync Time\" /tr schtasks /run /sc daily /st 00:00:00\n",
      "\n",
      "æ¥ç€ï¼Œæ‚¨å¯ä»¥è¿è¡Œè¿™ä¸ªä»»åŠ¡ï¼Œä½¿Windowsç³»ç»Ÿæ¯å¤©0ç‚¹è‡ªåŠ¨æ ¡å‡†æ—¶é—´ï¼š\n",
      "\n",
      "schtasks /run /tn \"Auto Sync Time\"\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "LinuxExpert: {'input': 'å¸®æˆ‘å†™ä¸ªcronè„šæœ¬ï¼Œè®©ç³»ç»Ÿæ¯å¤©0ç‚¹è‡ªåŠ¨é‡å¯'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ç­”æ¡ˆ:\n",
      "è¿™æ˜¯ä¸€ä¸ªLinux Shellè„šæœ¬ï¼Œå¯ä»¥åœ¨crontabä¸­ä½¿ç”¨ï¼š\n",
      "\n",
      "0 0 * * * /sbin/reboot\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "windows_template = \"\"\"\n",
    "ä½ åªä¼šå†™DOSæˆ–Windows Shellè„šæœ¬ã€‚ä½ ä¸ä¼šå†™ä»»ä½•å…¶ä»–è¯­è¨€çš„ç¨‹åºã€‚ä½ ä¹Ÿä¸ä¼šå†™Linuxè„šæœ¬ã€‚\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "linux_template = \"\"\"\n",
    "ä½ åªä¼šå†™Linux Shellè„šæœ¬ã€‚ä½ ä¸ä¼šå†™ä»»ä½•å…¶ä»–è¯­è¨€çš„ç¨‹åºã€‚ä½ ä¹Ÿä¸ä¼šå†™Windowsè„šæœ¬ã€‚\n",
    "\n",
    "ç”¨æˆ·é—®é¢˜:\n",
    "{input}\n",
    "\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"WindowsExpert\",\n",
    "        \"description\": \"æ“…é•¿å›ç­”Windows Shellç›¸å…³é—®é¢˜\",\n",
    "        \"prompt_template\": windows_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"LinuxExpert\",\n",
    "        \"description\": \"æ“…é•¿å›ç­”Linux Shellç›¸å…³é—®é¢˜\",\n",
    "        \"prompt_template\": linux_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "llm = OpenAI()\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = PromptTemplate(template=prompt_template,\n",
    "                            input_variables=[\"input\"])\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain\n",
    "default_chain = ConversationChain(llm=llm, output_key=\"text\")\n",
    "\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str)\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(\n",
    "    router_chain=router_chain,\n",
    "    destination_chains=destination_chains,\n",
    "    default_chain=default_chain,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(chain.run(\"å¸®æˆ‘å†™ä¸ªè„šæœ¬ï¼Œè®©Windowsç³»ç»Ÿæ¯å¤©0ç‚¹è‡ªåŠ¨æ ¡å¯¹æ—¶é—´\"))\n",
    "\n",
    "print(chain.run(\"å¸®æˆ‘å†™ä¸ªcronè„šæœ¬ï¼Œè®©ç³»ç»Ÿæ¯å¤©0ç‚¹è‡ªåŠ¨é‡å¯\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>Routeræ˜¯å¦æ˜¯ä¸€ä¸ªå¿…è¦çš„åŸºç¡€ç±»å‹ï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 å°è£…APIè°ƒç”¨ï¼šAPIChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new APIChain chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mhttps://api.open-meteo.com/v1/forecast?latitude=39.9042&longitude=116.4074&hourly=temperature_2m&temperature_unit=celsius\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m{\"latitude\":39.875,\"longitude\":116.375,\"generationtime_ms\":0.11909008026123047,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":47.0,\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"Â°C\"},\"hourly\":{\"time\":[\"2023-07-27T00:00\",\"2023-07-27T01:00\",\"2023-07-27T02:00\",\"2023-07-27T03:00\",\"2023-07-27T04:00\",\"2023-07-27T05:00\",\"2023-07-27T06:00\",\"2023-07-27T07:00\",\"2023-07-27T08:00\",\"2023-07-27T09:00\",\"2023-07-27T10:00\",\"2023-07-27T11:00\",\"2023-07-27T12:00\",\"2023-07-27T13:00\",\"2023-07-27T14:00\",\"2023-07-27T15:00\",\"2023-07-27T16:00\",\"2023-07-27T17:00\",\"2023-07-27T18:00\",\"2023-07-27T19:00\",\"2023-07-27T20:00\",\"2023-07-27T21:00\",\"2023-07-27T22:00\",\"2023-07-27T23:00\",\"2023-07-28T00:00\",\"2023-07-28T01:00\",\"2023-07-28T02:00\",\"2023-07-28T03:00\",\"2023-07-28T04:00\",\"2023-07-28T05:00\",\"2023-07-28T06:00\",\"2023-07-28T07:00\",\"2023-07-28T08:00\",\"2023-07-28T09:00\",\"2023-07-28T10:00\",\"2023-07-28T11:00\",\"2023-07-28T12:00\",\"2023-07-28T13:00\",\"2023-07-28T14:00\",\"2023-07-28T15:00\",\"2023-07-28T16:00\",\"2023-07-28T17:00\",\"2023-07-28T18:00\",\"2023-07-28T19:00\",\"2023-07-28T20:00\",\"2023-07-28T21:00\",\"2023-07-28T22:00\",\"2023-07-28T23:00\",\"2023-07-29T00:00\",\"2023-07-29T01:00\",\"2023-07-29T02:00\",\"2023-07-29T03:00\",\"2023-07-29T04:00\",\"2023-07-29T05:00\",\"2023-07-29T06:00\",\"2023-07-29T07:00\",\"2023-07-29T08:00\",\"2023-07-29T09:00\",\"2023-07-29T10:00\",\"2023-07-29T11:00\",\"2023-07-29T12:00\",\"2023-07-29T13:00\",\"2023-07-29T14:00\",\"2023-07-29T15:00\",\"2023-07-29T16:00\",\"2023-07-29T17:00\",\"2023-07-29T18:00\",\"2023-07-29T19:00\",\"2023-07-29T20:00\",\"2023-07-29T21:00\",\"2023-07-29T22:00\",\"2023-07-29T23:00\",\"2023-07-30T00:00\",\"2023-07-30T01:00\",\"2023-07-30T02:00\",\"2023-07-30T03:00\",\"2023-07-30T04:00\",\"2023-07-30T05:00\",\"2023-07-30T06:00\",\"2023-07-30T07:00\",\"2023-07-30T08:00\",\"2023-07-30T09:00\",\"2023-07-30T10:00\",\"2023-07-30T11:00\",\"2023-07-30T12:00\",\"2023-07-30T13:00\",\"2023-07-30T14:00\",\"2023-07-30T15:00\",\"2023-07-30T16:00\",\"2023-07-30T17:00\",\"2023-07-30T18:00\",\"2023-07-30T19:00\",\"2023-07-30T20:00\",\"2023-07-30T21:00\",\"2023-07-30T22:00\",\"2023-07-30T23:00\",\"2023-07-31T00:00\",\"2023-07-31T01:00\",\"2023-07-31T02:00\",\"2023-07-31T03:00\",\"2023-07-31T04:00\",\"2023-07-31T05:00\",\"2023-07-31T06:00\",\"2023-07-31T07:00\",\"2023-07-31T08:00\",\"2023-07-31T09:00\",\"2023-07-31T10:00\",\"2023-07-31T11:00\",\"2023-07-31T12:00\",\"2023-07-31T13:00\",\"2023-07-31T14:00\",\"2023-07-31T15:00\",\"2023-07-31T16:00\",\"2023-07-31T17:00\",\"2023-07-31T18:00\",\"2023-07-31T19:00\",\"2023-07-31T20:00\",\"2023-07-31T21:00\",\"2023-07-31T22:00\",\"2023-07-31T23:00\",\"2023-08-01T00:00\",\"2023-08-01T01:00\",\"2023-08-01T02:00\",\"2023-08-01T03:00\",\"2023-08-01T04:00\",\"2023-08-01T05:00\",\"2023-08-01T06:00\",\"2023-08-01T07:00\",\"2023-08-01T08:00\",\"2023-08-01T09:00\",\"2023-08-01T10:00\",\"2023-08-01T11:00\",\"2023-08-01T12:00\",\"2023-08-01T13:00\",\"2023-08-01T14:00\",\"2023-08-01T15:00\",\"2023-08-01T16:00\",\"2023-08-01T17:00\",\"2023-08-01T18:00\",\"2023-08-01T19:00\",\"2023-08-01T20:00\",\"2023-08-01T21:00\",\"2023-08-01T22:00\",\"2023-08-01T23:00\",\"2023-08-02T00:00\",\"2023-08-02T01:00\",\"2023-08-02T02:00\",\"2023-08-02T03:00\",\"2023-08-02T04:00\",\"2023-08-02T05:00\",\"2023-08-02T06:00\",\"2023-08-02T07:00\",\"2023-08-02T08:00\",\"2023-08-02T09:00\",\"2023-08-02T10:00\",\"2023-08-02T11:00\",\"2023-08-02T12:00\",\"2023-08-02T13:00\",\"2023-08-02T14:00\",\"2023-08-02T15:00\",\"2023-08-02T16:00\",\"2023-08-02T17:00\",\"2023-08-02T18:00\",\"2023-08-02T19:00\",\"2023-08-02T20:00\",\"2023-08-02T21:00\",\"2023-08-02T22:00\",\"2023-08-02T23:00\"],\"temperature_2m\":[27.6,28.8,30.7,32.4,33.9,35.0,34.6,34.6,34.3,34.0,33.3,32.6,31.5,27.8,27.6,27.4,26.9,26.5,26.2,26.1,26.1,25.9,25.4,25.3,25.7,26.1,26.9,27.0,26.1,26.0,27.3,28.1,28.4,28.3,28.0,27.8,27.3,26.9,26.5,26.3,26.0,25.7,25.5,25.4,25.3,25.2,25.1,25.2,25.4,26.2,26.8,26.7,27.1,27.7,28.2,28.2,27.9,27.1,26.2,25.7,25.4,25.1,24.9,24.8,24.7,24.6,24.4,24.4,24.2,24.1,24.1,24.2,24.2,24.4,24.5,24.5,24.6,24.8,23.7,23.8,24.0,23.9,23.8,23.8,24.0,24.0,23.8,23.7,23.7,23.6,23.6,23.5,23.3,23.2,23.2,23.1,23.2,23.3,23.4,23.6,23.8,24.1,24.4,24.7,25.1,25.4,25.6,25.7,25.7,25.6,25.5,25.3,25.2,25.1,25.1,25.0,24.9,24.9,25.1,25.3,25.5,25.8,26.2,26.4,26.6,26.6,26.6,28.1,28.3,28.3,28.0,27.4,26.8,26.5,26.3,26.0,25.8,25.5,25.3,25.1,25.0,25.1,25.4,25.9,26.7,27.9,29.3,30.6,31.8,32.8,33.6,34.1,34.3,34.1,33.1,31.7,30.4,29.7,29.1,28.6,28.1,27.6,27.2,26.9,26.7,26.7,27.1,27.6]}}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The temperature in Beijing today is 27.6Â°C.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import APIChain\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "from langchain.chains.api import open_meteo_docs\n",
    "chain_new = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
    "chain_new.run('åŒ—äº¬ä»Šå¤©æ°”æ¸©ï¼Œæ‘„æ°åº¦')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8 è°ƒç”¨OpenAI Function Callingè·å¾—Pydanticè¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a world class algorithm for extracting information in structured formats.\n",
      "Human: Use the given format to extract information from the following input:\n",
      "Human: å¯„ç»™äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼çš„ç‹å“ç„¶ï¼Œ13012345678\n",
      "Human: Tips: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Contact(name='ç‹å“ç„¶', address='äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼', tel='13012345678')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "from langchain.chains.openai_functions import (\n",
    "    create_openai_fn_chain,\n",
    "    create_structured_output_chain,\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "class Contact(BaseModel):\n",
    "    \"\"\"Extracting information about a contact persion.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    address: str = Field(..., description=\"The person's address\")\n",
    "    tel: str = Field(None, description=\"The person's telephone/mobile number\")\n",
    "\n",
    "prompt_msgs = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a world class algorithm for extracting information in structured formats.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"Use the given format to extract information from the following input:\"\n",
    "    ),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
    "    HumanMessage(content=\"Tips: Make sure to answer in the correct format\"),\n",
    "]\n",
    "prompt = ChatPromptTemplate(messages=prompt_msgs)\n",
    "llm = ChatOpenAI(model=\"gpt-4-0613\", temperature=0)\n",
    "\n",
    "chain = create_structured_output_chain(Contact, llm, prompt, verbose=True)\n",
    "\n",
    "chain.run(\"å¯„ç»™äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼çš„ç‹å“ç„¶ï¼Œ13012345678\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a world class algorithm for extracting information in structured formats.\n",
      "Human: Use the given format to extract information from the following input:\n",
      "Human: å¯„ç»™äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼çš„ç‹å“ç„¶ï¼Œ13012345678\n",
      "Human: Tips: Make sure to answer in the correct format\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mname='ç‹å“ç„¶' address='äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼' tel='13012345678'\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mBEGIN:VCARD\n",
      "VERSION:2.1\n",
      "N:ç‹å“ç„¶\n",
      "ADR:äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼\n",
      "TEL:13012345678\n",
      "END:VCARD\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "BEGIN:VCARD\n",
      "VERSION:2.1\n",
      "N:ç‹å“ç„¶\n",
      "ADR:äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼\n",
      "TEL:13012345678\n",
      "END:VCARD\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from typing import Dict\n",
    "\n",
    "def process(inputs: Dict[str,Contact])->str:\n",
    "    person = inputs[\"contact\"]\n",
    "    return {\"text\":f\"BEGIN:VCARD\\nVERSION:2.1\\nN:{person.name}\\nADR:{person.address}\\nTEL:{person.tel}\\nEND:VCARD\"}\n",
    "\n",
    "\n",
    "transform_chain = TransformChain(\n",
    "    input_variables=[\"contact\"], output_variables=[\"text\"], transform=process\n",
    ")\n",
    "\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[chain, transform_chain], verbose=True)\n",
    "\n",
    "print(overall_chain.run(\"å¯„ç»™äº®é©¬æ¡¥å¤–äº¤åŠå…¬å¤§æ¥¼çš„ç‹å“ç„¶ï¼Œ13012345678\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9 åŸºäº Document çš„ Chains\n",
    "\n",
    "<img src=\"stuff.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"refine.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"map_reduce.jpg\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"map_rerank.jpg\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new MapRerankDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "is June 19, 2023. Note that the paper needs to be fully reviewedby ARR in order to make a commitment, thus the latest date for ARR submission will be April15, 2023.\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "have to fill in the submission form in the Softconf/START system and upload an initialpdf of their papers before May 15, 2023 (23:59 GMT-11).Â The submission link is https://softconf.com/n/sigdial2023/ .Submission via ACL Rolling Review (ARR)Please refer to the ARR Call for Papers  for detailed information about submission guidelines toARR. The commitment deadline for authors to submit their reviewed papers, reviews, andmeta-review to SIGDIAL 2023 is June 19, 2023. Note that the paper needs to\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "whichare available as an Overleaf template  and also downloadable directly  (Latex and Word)Submissions must conform to the official ACL style guidelines, which are contained in thesetemplates. Submissions must be electronic, in PDF format.Submission DeadlineSIGDIAL will accept regular submissions through the Softconf/START system, as well ascommitment of already reviewed papers through the ACL Rolling Review (ARR) system.Regular submissionAuthors have to fill in the submission form in the\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mUse the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "In addition to giving an answer, also return a score of how fully it answered the user's question. This should be in the following format:\n",
      "\n",
      "Question: [question here]\n",
      "Helpful Answer: [answer here]\n",
      "Score: [score between 0 and 100]\n",
      "\n",
      "How to determine the score:\n",
      "- Higher is a better answer\n",
      "- Better responds fully to the asked question, with sufficient level of detail\n",
      "- If you do not know the answer based on the context, that should be a score of 0\n",
      "- Don't be overconfident!\n",
      "\n",
      "Example #1\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Apples are red\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: red\n",
      "Score: 100\n",
      "\n",
      "Example #2\n",
      "\n",
      "Context:\n",
      "---------\n",
      "it was night and the witness forgot his glasses. he was not sure if it was a sports car or an suv\n",
      "---------\n",
      "Question: what type was the car?\n",
      "Helpful Answer: a sports car or an suv\n",
      "Score: 60\n",
      "\n",
      "Example #3\n",
      "\n",
      "Context:\n",
      "---------\n",
      "Pears are either red or orange\n",
      "---------\n",
      "Question: what color are apples?\n",
      "Helpful Answer: This document does not answer the question\n",
      "Score: 0\n",
      "\n",
      "Begin!\n",
      "\n",
      "Context:\n",
      "---------\n",
      "There will be 1 extra week given to the authors for updating the paper after the submission.Use the following link to commit the paper: https://forms.office.com/r/wxZHZ08iZmMentoringAcceptable submissions that require language (English) or organizational assistance will beflagged for mentoring, and accepted with a recommendation to revise with the help of amentor. An experienced mentor who has previously published in the SIGDIAL venue will thenhelp the authors of these flagged papers prepare\n",
      "---------\n",
      "Question: When is the regular submission deadline? When is the ARR submission deadline?\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The regular submission deadline is April 15, 2023. The ARR submission deadline is also April 15, 2023.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def set_verbose_recusively(chain):\n",
    "    chain.verbose = True\n",
    "    for attr in dir(chain):\n",
    "        if attr.endswith('_chain') and isinstance(getattr(chain,attr),Chain):\n",
    "            subchain=getattr(chain,attr)\n",
    "            set_verbose_recusively(subchain)\n",
    "\n",
    "loader = PyPDFLoader(\"SIGDIAL2023.pdf\")\n",
    "documents = loader.load_and_split()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "paragraphs = text_splitter.create_documents(\n",
    "    [d.page_content for d in documents])\n",
    "# print(paragraphs)\n",
    "embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "db = FAISS.from_documents(paragraphs, embeddings)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    chain_type=\"map_rerank\",\n",
    "    retriever=db.as_retriever(),\n",
    "    verbose=True\n",
    ")\n",
    "set_verbose_recusively(qa_chain)\n",
    "\n",
    "query = \"When is the regular submission deadline? When is the ARR submission deadline?\"\n",
    "qa_chain.run(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äº”ã€æ™ºèƒ½ä½“æ¶æ„ï¼šAgent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 ä»€ä¹ˆæ˜¯æ™ºèƒ½ä½“ï¼ˆAgentï¼‰\n",
    "\n",
    "å°†å¤§è¯­è¨€æ¨¡å‹ä½œä¸ºä¸€ä¸ªæ¨ç†å¼•æ“ã€‚ç»™å®šä¸€ä¸ªä»»åŠ¡ï¼Œæ™ºèƒ½ä½“è‡ªåŠ¨ç”Ÿæˆå®Œæˆä»»åŠ¡æ‰€éœ€çš„æ­¥éª¤ï¼Œæ‰§è¡Œç›¸åº”åŠ¨ä½œï¼ˆä¾‹å¦‚é€‰æ‹©å¹¶è°ƒç”¨å·¥å…·ï¼‰ï¼Œç›´åˆ°ä»»åŠ¡å®Œæˆã€‚\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 å…ˆå®šä¹‰ä¸€äº›å·¥å…·ï¼šTools\n",
    "\n",
    "- å¯ä»¥æ˜¯ä¸€ä¸ªå‡½æ•°æˆ–ä¸‰æ–¹ API\n",
    "- ä¹Ÿå¯ä»¥æŠŠä¸€ä¸ª Chain æˆ–è€… Agent çš„ run()ä½œä¸ºä¸€ä¸ª Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool.from_function(\n",
    "        func=search.run,\n",
    "        name=\"Search\",\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import Tool, tool\n",
    "import calendar\n",
    "import dateutil.parser as parser\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "@tool(\"weekday\")\n",
    "def weekday(date_str: str) -> str:\n",
    "    \"\"\"Convert date to weekday name\"\"\"\n",
    "    d = parser.parse(date_str)\n",
    "    return calendar.day_name[d.weekday()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools([\"serpapi\"])\n",
    "tools += [weekday]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 æ™ºèƒ½ä½“ç±»å‹ï¼šReAct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ReAct.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-search-results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mæˆ‘éœ€è¦çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥æ˜¯å“ªä¸€å¤©ï¼Œç„¶åæˆ‘å¯ä»¥ä½¿ç”¨weekdayå·¥å…·æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚\n",
      "Action: Search\n",
      "Action Input: å‘¨æ°ä¼¦ç”Ÿæ—¥æ˜¯å“ªä¸€å¤©\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mJanuary 18, 1979\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥æ˜¯1æœˆ18æ—¥ï¼Œ1979å¹´ã€‚æˆ‘å¯ä»¥ä½¿ç”¨weekdayå·¥å…·æ¥æ‰¾å‡ºé‚£å¤©æ˜¯æ˜ŸæœŸå‡ ã€‚\n",
      "Action: weekday\n",
      "Action Input: January 18, 1979\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mThursday\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mæˆ‘ç°åœ¨çŸ¥é“å‘¨æ°ä¼¦çš„ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå››ã€‚\n",
      "Final Answer: æ˜ŸæœŸå››\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æ˜ŸæœŸå››'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "agent.run(\"å‘¨æ°ä¼¦ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå‡ \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 é€šè¿‡ OpenAI Function Calling å®ç°æ™ºèƒ½ä½“\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `Search` with `å‘¨æ°ä¼¦çš„ç”Ÿæ—¥`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mJanuary 18, 1979\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `weekday` with `{'date_str': '1979-01-18'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mThursday\u001b[0m\u001b[32;1m\u001b[1;3må‘¨æ°ä¼¦çš„ç”Ÿæ—¥ï¼ˆ1979å¹´1æœˆ18æ—¥ï¼‰æ˜¯æ˜ŸæœŸå››ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'å‘¨æ°ä¼¦çš„ç”Ÿæ—¥ï¼ˆ1979å¹´1æœˆ18æ—¥ï¼‰æ˜¯æ˜ŸæœŸå››ã€‚'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import initialize_agent\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4-0613', temperature=0)\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.OPENAI_FUNCTIONS,\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    early_stopping_method=\"generate\",\n",
    ")\n",
    "agent.run(\"å‘¨æ°ä¼¦ç”Ÿæ—¥é‚£å¤©æ˜¯æ˜ŸæœŸå‡ \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 æ™ºèƒ½ä½“ç±»å‹ï¼šSelfAskWithSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m Yes.\n",
      "Follow up: Who is the wife of Feng Xiaogang?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mHe married actress Xu Fan in 1999. FilmographyEdit. As directorEdit. Year, English Title ...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mFollow up: What movies has Xu Fan acted in?\u001b[0m\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mXu Fan is a Chinese actress and Asian Film Awards winner. She married film director Feng Xiaogang in 1999 and has starred in a number of films and television series directed by her husband.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mSo the final answer is: Xu Fan has starred in a number of films and television series directed by her husband.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Xu Fan has starred in a number of films and television series directed by her husband.'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import OpenAI, SerpAPIWrapper\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Intermediate Answer\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to ask with search\",\n",
    "    )\n",
    "]\n",
    "\n",
    "self_ask_with_search = initialize_agent(\n",
    "    tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True\n",
    ")\n",
    "self_ask_with_search.run(\n",
    "    \"å†¯å°åˆšçš„è€å©†æ¼”è¿‡ä»€ä¹ˆç”µå½±\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 æ™ºèƒ½ä½“ç±»å‹ï¼šPlan-and-Execute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PlanExec.png\" style=\"margin-left: 0px\" width=500px>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-experimental\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new PlanAndExecute chain...\u001b[0m\n",
      "steps=[Step(value='Access a reliable weather forecasting website or API to gather the weather data for Beijing and Shanghai for tomorrow.'), Step(value='Analyze the weather data for both cities, focusing on key aspects such as temperature, humidity, wind speed, and weather conditions (sunny, cloudy, rainy, etc.).'), Step(value='Compare the weather data of the two cities, highlighting the similarities and differences.'), Step(value='Write a report in Chinese, summarizing the weather forecast for both cities and the comparison between them.'), Step(value='Review the report to ensure it is accurate and clear.'), Step(value='Given the above steps taken, please respond to the users original question.\\n')]\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The assistant needs to access a reliable weather forecasting website or API to gather the weather data for Beijing and Shanghai for tomorrow. However, the assistant does not have the capability to directly access external APIs or websites. It can only provide information on how to do it.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"I'm sorry for the misunderstanding, but I don't have the capability to directly access external APIs or websites. However, I can guide you on how to do it. You can use weather forecasting APIs like OpenWeatherMap, Weatherstack, or AccuWeather. These APIs allow you to access weather data for any location. You would need to sign up, get an API key, and make a GET request to their servers with the API key and the name of the city as parameters. The response will usually be in JSON format, which you can parse to get the weather data.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Access a reliable weather forecasting website or API to gather the weather data for Beijing and Shanghai for tomorrow.\n",
      "\n",
      "Response: I'm sorry for the misunderstanding, but I don't have the capability to directly access external APIs or websites. However, I can guide you on how to do it. You can use weather forecasting APIs like OpenWeatherMap, Weatherstack, or AccuWeather. These APIs allow you to access weather data for any location. You would need to sign up, get an API key, and make a GET request to their servers with the API key and the name of the city as parameters. The response will usually be in JSON format, which you can parse to get the weather data.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user wants to analyze the weather data for Beijing and Shanghai, focusing on key aspects such as temperature, humidity, wind speed, and weather conditions. Since I don't have the capability to directly access external APIs or websites, I can guide the user on how to analyze the data once they have it.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Once you have the weather data in JSON format, you can analyze it by extracting the key aspects you're interested in. Here's a general idea of how you might do it:\\n\\n1. Temperature: This is usually given in degrees Celsius or Fahrenheit. You might want to compare the temperatures of the two cities and see which one is hotter or colder.\\n\\n2. Humidity: This is given as a percentage. A higher percentage means the air is more humid. You can compare the humidity levels of the two cities.\\n\\n3. Wind Speed: This is usually given in meters per second or miles per hour. You can compare the wind speeds of the two cities to see which one is windier.\\n\\n4. Weather Conditions: This is usually a descriptive term like 'sunny', 'cloudy', 'rainy', etc. You can see what the weather conditions are like in each city.\\n\\nRemember, the exact way to extract this information will depend on the specific structure of the JSON data you receive from the weather API.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Analyze the weather data for both cities, focusing on key aspects such as temperature, humidity, wind speed, and weather conditions (sunny, cloudy, rainy, etc.).\n",
      "\n",
      "Response: Once you have the weather data in JSON format, you can analyze it by extracting the key aspects you're interested in. Here's a general idea of how you might do it:\n",
      "\n",
      "1. Temperature: This is usually given in degrees Celsius or Fahrenheit. You might want to compare the temperatures of the two cities and see which one is hotter or colder.\n",
      "\n",
      "2. Humidity: This is given as a percentage. A higher percentage means the air is more humid. You can compare the humidity levels of the two cities.\n",
      "\n",
      "3. Wind Speed: This is usually given in meters per second or miles per hour. You can compare the wind speeds of the two cities to see which one is windier.\n",
      "\n",
      "4. Weather Conditions: This is usually a descriptive term like 'sunny', 'cloudy', 'rainy', etc. You can see what the weather conditions are like in each city.\n",
      "\n",
      "Remember, the exact way to extract this information will depend on the specific structure of the JSON data you receive from the weather API.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user wants to compare the weather data of Beijing and Shanghai. Since I don't have the actual data, I can only provide a general approach on how to compare the data.\n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"To compare the weather data of Beijing and Shanghai, you can create a table or a chart with the key aspects you're interested in (temperature, humidity, wind speed, and weather conditions). For each aspect, note the values for both cities. Then, you can highlight the similarities and differences. For example, if both cities have similar temperatures but different humidity levels, that's a noteworthy point. Similarly, if both cities are sunny but one has a much higher wind speed, that's another interesting comparison. Remember, the goal is to draw meaningful insights from the data, so focus on the aspects that you find most revealing about the weather in these two cities.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Compare the weather data of the two cities, highlighting the similarities and differences.\n",
      "\n",
      "Response: To compare the weather data of Beijing and Shanghai, you can create a table or a chart with the key aspects you're interested in (temperature, humidity, wind speed, and weather conditions). For each aspect, note the values for both cities. Then, you can highlight the similarities and differences. For example, if both cities have similar temperatures but different humidity levels, that's a noteworthy point. Similarly, if both cities are sunny but one has a much higher wind speed, that's another interesting comparison. Remember, the goal is to draw meaningful insights from the data, so focus on the aspects that you find most revealing about the weather in these two cities.\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: The user is asking for a report summarizing the weather forecast for Beijing and Shanghai in Chinese. However, I don't have the actual weather data for these cities. I can provide a template for how such a report could be structured in Chinese. \n",
      "\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤©æ°”é¢„æŠ¥æŠ¥å‘Šçš„æ¨¡æ¿ï¼š\\n\\næ ‡é¢˜ï¼šåŒ—äº¬å’Œä¸Šæµ·çš„å¤©æ°”é¢„æŠ¥åŠå…¶æ¯”è¾ƒ\\n\\nä¸€ã€åŒ—äº¬çš„å¤©æ°”é¢„æŠ¥\\næ˜å¤©ï¼ŒåŒ—äº¬çš„å¤©æ°”å°†æ˜¯______ã€‚é¢„è®¡æœ€é«˜æ¸©åº¦ä¸º______åº¦ï¼Œæœ€ä½æ¸©åº¦ä¸º______åº¦ã€‚æ¹¿åº¦ä¸º______%ï¼Œé£é€Ÿä¸º______ã€‚\\n\\näºŒã€ä¸Šæµ·çš„å¤©æ°”é¢„æŠ¥\\næ˜å¤©ï¼Œä¸Šæµ·çš„å¤©æ°”å°†æ˜¯______ã€‚é¢„è®¡æœ€é«˜æ¸©åº¦ä¸º______åº¦ï¼Œæœ€ä½æ¸©åº¦ä¸º______åº¦ã€‚æ¹¿åº¦ä¸º______%ï¼Œé£é€Ÿä¸º______ã€‚\\n\\nä¸‰ã€åŒ—äº¬å’Œä¸Šæµ·çš„å¤©æ°”æ¯”è¾ƒ\\nåœ¨æ¸©åº¦æ–¹é¢ï¼Œ______ã€‚åœ¨æ¹¿åº¦æ–¹é¢ï¼Œ______ã€‚åœ¨é£é€Ÿæ–¹é¢ï¼Œ______ã€‚æ€»çš„æ¥è¯´ï¼Œ______ã€‚\\n\\nç»“è®ºï¼š______ã€‚\\n\\nè¯·å°†ä¸Šè¿°ç©ºç™½å¤„å¡«å…¥ç›¸åº”çš„å¤©æ°”æ•°æ®ã€‚\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "*****\n",
      "\n",
      "Step: Write a report in Chinese, summarizing the weather forecast for both cities and the comparison between them.\n",
      "\n",
      "Response: ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¤©æ°”é¢„æŠ¥æŠ¥å‘Šçš„æ¨¡æ¿ï¼š\n",
      "\n",
      "æ ‡é¢˜ï¼šåŒ—äº¬å’Œä¸Šæµ·çš„å¤©æ°”é¢„æŠ¥åŠå…¶æ¯”è¾ƒ\n",
      "\n",
      "ä¸€ã€åŒ—äº¬çš„å¤©æ°”é¢„æŠ¥\n",
      "æ˜å¤©ï¼ŒåŒ—äº¬çš„å¤©æ°”å°†æ˜¯______ã€‚é¢„è®¡æœ€é«˜æ¸©åº¦ä¸º______åº¦ï¼Œæœ€ä½æ¸©åº¦ä¸º______åº¦ã€‚æ¹¿åº¦ä¸º______%ï¼Œé£é€Ÿä¸º______ã€‚\n",
      "\n",
      "äºŒã€ä¸Šæµ·çš„å¤©æ°”é¢„æŠ¥\n",
      "æ˜å¤©ï¼Œä¸Šæµ·çš„å¤©æ°”å°†æ˜¯______ã€‚é¢„è®¡æœ€é«˜æ¸©åº¦ä¸º______åº¦ï¼Œæœ€ä½æ¸©åº¦ä¸º______åº¦ã€‚æ¹¿åº¦ä¸º______%ï¼Œé£é€Ÿä¸º______ã€‚\n",
      "\n",
      "ä¸‰ã€åŒ—äº¬å’Œä¸Šæµ·çš„å¤©æ°”æ¯”è¾ƒ\n",
      "åœ¨æ¸©åº¦æ–¹é¢ï¼Œ______ã€‚åœ¨æ¹¿åº¦æ–¹é¢ï¼Œ______ã€‚åœ¨é£é€Ÿæ–¹é¢ï¼Œ______ã€‚æ€»çš„æ¥è¯´ï¼Œ______ã€‚\n",
      "\n",
      "ç»“è®ºï¼š______ã€‚\n",
      "\n",
      "è¯·å°†ä¸Šè¿°ç©ºç™½å¤„å¡«å…¥ç›¸åº”çš„å¤©æ°”æ•°æ®ã€‚\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from langchain.utilities.wolfram_alpha import WolframAlphaAPIWrapper\n",
    "from langchain.agents import load_tools\n",
    "from langchain import SerpAPIWrapper\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-4', temperature=0)\n",
    "\n",
    "search = SerpAPIWrapper(params={\n",
    "    'engine': 'google', \n",
    "    'gl': 'cn', \n",
    "    'google_domain': 'google.com.hk', \n",
    "    'hl': 'zh-cn'\n",
    "})\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\"\n",
    "    )\n",
    "]\n",
    "\n",
    "planner = load_chat_planner(llm)\n",
    "executor = load_agent_executor(llm, tools, verbose=True)\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True)\n",
    "\n",
    "agent.run(\"åˆ†æåŒ—äº¬æ˜å¤©å¤©æ°”ï¼Œä¸ä¸Šæµ·æ˜å¤©å¤©æ°”å¯¹æ¯”ï¼Œç”¨ä¸­æ–‡å†™ä¸€éæŠ¥å‘Š\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…­ã€Callbacks\n",
    "\n",
    "å›è°ƒå‡½æ•°ï¼Œç”¨äºç›‘æµ‹ã€è®°å½•è°ƒç”¨è¿‡ç¨‹ä¸­çš„ä¿¡æ¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseCallbackHandler:\n",
    "    \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM starts running.\"\"\"\n",
    "\n",
    "    def on_chat_model_start(\n",
    "        self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when Chat Model starts running.\"\"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM ends running.\"\"\"\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain starts running.\"\"\"\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when chain ends running.\"\"\"\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when chain errors.\"\"\"\n",
    "\n",
    "    def on_tool_start(\n",
    "        self, serialized: Dict[str, Any], input_str: str, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool starts running.\"\"\"\n",
    "\n",
    "    def on_tool_end(self, output: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when tool ends running.\"\"\"\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run when tool errors.\"\"\"\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on arbitrary text.\"\"\"\n",
    "\n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "\n",
    "    def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:\n",
    "        \"\"\"Run on agent end.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain Start: {'number': 1}\n",
      "On text: Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 1 = \u001b[0m\n",
      "Done!\n",
      "Chain Start: {'number': 2}\n",
      "On text: Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m1 + 2 = \u001b[0m\n",
      "Feed LLM with ['1 + 2 = ']\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class myhandler(BaseCallbackHandler):\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"Feed LLM with {prompts}\")\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> Any:\n",
    "        print(f\"Chain Start: {inputs}\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        print(f\"Done!\")\n",
    "\n",
    "    def on_text(self, text: str, **kwargs: Any) -> Any:\n",
    "        print(f\"On text: {text}\")\n",
    "\n",
    "\n",
    "handler = myhandler()\n",
    "\n",
    "llm = OpenAI()\n",
    "prompt = PromptTemplate.from_template(\"1 + {number} = \")\n",
    "\n",
    "# åœ¨æ„é€ çš„æ—¶å€™åŠ å›è°ƒï¼Œåªè§¦å‘è¿™ä¸ªå¯¹è±¡å¯¹åº”çš„äº‹ä»¶\n",
    "chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler])\n",
    "x = chain.run(number=1)\n",
    "\n",
    "# åœ¨è¿è¡Œçš„æ—¶å€™åŠ å›è°ƒï¼Œè§¦å‘è¿™ä¸ªè¿‡ç¨‹çš„æ‰€æœ‰å­è¿‡ç¨‹çš„äº‹ä»¶\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "x = chain.run(number=2, callbacks=[handler])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¤§æ¨¡å‹æ—¶ä»£è½¯ä»¶çš„æ¼”å˜è¶‹åŠ¿\n",
    "\n",
    "<img src=\"agent.png\" style=\"margin-left: 0px\" width=600px>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b>\n",
    "<ul>\n",
    "<li>ä»è½¯ä»¶å·¥ç¨‹çš„è§’åº¦ï¼ŒLangChainç°é˜¶æ®µçš„ç¼ºç‚¹æ˜¯ä»€ä¹ˆ</li>\n",
    "<li>è·ç¦»æ™ºèƒ½ä½“å¤§è§„æ¨¡åº”ç”¨ï¼Œæˆ‘ä»¬è¿˜æœ‰ä»€ä¹ˆæ²¡è§£å†³çš„é—®é¢˜</li>\n",
    "<li>ä½ è§‰å¾—æ™ºèƒ½ä½“æœ‰å“ªäº›å¯ä¼˜åŒ–çš„æ–¹å‘</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangFlow\n",
    "\n",
    "<img src=\"https://github.com/logspace-ai/langflow/raw/main/img/langflow-demo.gif?raw=true\" style=\"margin-left: 0px\">\n",
    "\n",
    "https://github.com/logspace-ai/langflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½œä¸š\n",
    "\n",
    "åšä¸ªè‡ªå·±çš„ [ChatPDF(https://www.chatpdf.com/)](https://www.chatpdf.com/)ï¼ˆéœ€ç§‘å­¦è®¿é—®ï¼‰ã€‚éœ€æ±‚ï¼š\n",
    "\n",
    "1. ä»æœ¬åœ°åŠ è½½ PDF æ–‡ä»¶ï¼ŒåŸºäº PDF çš„å†…å®¹å¯¹è¯\n",
    "2. å¯ä»¥æ— å‰ç«¯ï¼Œåªè¦èƒ½åœ¨å‘½ä»¤è¡Œè¿è¡Œå°±è¡Œ\n",
    "3. å…¶å®ƒéšæ„å‘æŒ¥\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¯¾åè°ƒæŸ¥\n",
    "\n",
    "è¯·ç‚¹å‡»é“¾æ¥æˆ–æ‰«ç å¡«å†™é—®å·ï¼Œå¸®åŠ©æˆ‘ä»¬æŒç»­æ”¹è¿›è¯¾ç¨‹å†…å®¹ã€‚è°¢è°¢ï¼\n",
    "\n",
    "https://agiclass.feishu.cn/share/base/form/shrcnU6ywdMDS2caxf6gp7dYQ63\n",
    "\n",
    "<img src=\"../survey.png\" width=\"200\" style=\"margin-left: 0px\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
