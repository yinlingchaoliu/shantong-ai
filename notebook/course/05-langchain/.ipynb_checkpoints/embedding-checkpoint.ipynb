{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4c42f74-3c81-4965-87e9-62ffe2fe24b2",
   "metadata": {},
   "source": [
    "## 💡展开说说 Embeddings\n",
    "\n",
    "1. 未来的开发中，我们会频繁用到Embedding\n",
    "2. 不止是基于大模型的应用，其它场景中Embedding也非常常用，比如「聚类」\n",
    "3. Transformer的输入就是Embedding，其隐层也可以当做Embedding来用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511c1e3-edb8-434a-92c8-9d4b7ab18902",
   "metadata": {},
   "source": [
    "## 什么是Embeddings\n",
    "\n",
    "### 从词向量说起\n",
    "\n",
    "从字面本身计算语义相关性是不够的\n",
    "- 不同字，同义：「快乐」vs.「高兴」\n",
    "- 同字，不同义：「上马」vs.「马上」\n",
    "\n",
    "所以我们需要一种方法，能够有效计算词与词之间的关系，词向量（Word Embedding）应运而生\n",
    "\n",
    "<img src=\"word2vec.png\" style=\"margin-left: 0px\" width=500px>\n",
    "<img src=\"word2vec2.png\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "<br/>\n",
    "\n",
    "### 词向量的基本原理：用一个词上下文窗口表示它自身\n",
    "\n",
    "<br/>\n",
    "<img src=\"w2v.png\" style=\"margin-left: 0px\" width=500px>\n",
    "\n",
    "### 词向量的不足\n",
    "\n",
    "- 同一个词在不同上下文中语义不同：我从「马上」下来 vs. 我「马上」下来"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52dccc1-f159-4995-8036-479f76ca48ff",
   "metadata": {},
   "source": [
    "### 基于整个句子，表示句中每个词，那么同时我们也就表示了整个句子\n",
    "\n",
    "<br />\n",
    "<img src=\"mlm.png\" style=\"margin-left: 0px\" width=500px>\n",
    "<br />\n",
    "\n",
    "### 所以，句子、篇章都可以向量化\n",
    "\n",
    "<br />\n",
    "<img src=\"SemanticSearch.png\" style=\"margin-left: 0px\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e9fce-4932-4d56-ab09-d7d29b037e23",
   "metadata": {},
   "source": [
    "### 向量相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd74d6f2-f2ec-41b4-bdff-2f0d88e53b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine distance:\n",
      "1.0\n",
      "0.7806203619322558\n",
      "0.7926144228096119\n",
      "0.7616280759223305\n",
      "0.7182043590648329\n",
      "0.7306356855118128\n",
      "\n",
      "Euclidean distance:\n",
      "0.0\n",
      "0.662389062540602\n",
      "0.6440272888198665\n",
      "0.6904663943141007\n",
      "0.7507271636300279\n",
      "0.7339813496648758\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))\n",
    "\n",
    "def l2(a,b):\n",
    "    x = np.asarray(a)-np.asarray(b)\n",
    "    return norm(x)\n",
    "\n",
    "model = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "#oc_embeddings = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "query = \"国际争端\"\n",
    "documents = [\n",
    "    \"联合国就苏丹达尔富尔地区大规模暴力事件发出警告\",\n",
    "    \"土耳其、芬兰、瑞典与北约代表将继续就瑞典“入约”问题进行谈判\",\n",
    "    \"日本岐阜市陆上自卫队射击场内发生枪击事件 3人受伤\",\n",
    "    \"国家游泳中心（水立方）：恢复游泳、嬉水乐园等水上项目运营\",\n",
    "    \"我国首次在空间站开展舱外辐射生物学暴露实验\",\n",
    "]\n",
    "\n",
    "query_vec = model.embed_query(query)\n",
    "doc_vecs = model.embed_documents(documents)\n",
    "\n",
    "print(\"Cosine distance:\") #越大越相似\n",
    "print(cos_sim(query_vec,query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(cos_sim(query_vec,vec))\n",
    "\n",
    "print(\"\\nEuclidean distance:\") #越小越相似\n",
    "print(l2(query_vec,query_vec))\n",
    "for vec in doc_vecs:\n",
    "    print(l2(query_vec,vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28542445-40a8-4246-a491-17b68f943484",
   "metadata": {},
   "source": [
    "### 基于相似度聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac59a165-09de-4eca-9b19-b689246f6277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t这个多少钱\n",
      "0\t啥价\n",
      "0\t给我报个价\n",
      "1\t我要红色的\n",
      "-1\t不要了\n",
      "-1\t算了\n",
      "1\t来红的吧\n",
      "-1\t作罢\n",
      "0\t价格介绍一下\n",
      "1\t红的这个给我吧\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "texts = [\n",
    "    \"这个多少钱\",\n",
    "    \"啥价\",\n",
    "    \"给我报个价\",\n",
    "    \"我要红色的\",\n",
    "    \"不要了\",\n",
    "    \"算了\",\n",
    "    \"来红的吧\",\n",
    "    \"作罢\",\n",
    "    \"价格介绍一下\",\n",
    "    \"红的这个给我吧\"\n",
    "]\n",
    "model = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "X = []\n",
    "for t in texts:\n",
    "    embedding = model.embed_query(t)\n",
    "    X.append(embedding)\n",
    "    \n",
    "#clusters = KMeans(n_clusters=3, random_state=42, n_init=\"auto\").fit(X)\n",
    "clusters = DBSCAN(eps=0.55, min_samples=2).fit(X)\n",
    "for i,t in enumerate(texts):\n",
    "    print(\"{}\\t{}\".format(clusters.labels_[i],t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b938d-edc0-4dcd-a6b8-ed4e9bc03206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
