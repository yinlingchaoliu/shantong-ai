{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¡ è¿™èŠ‚è¯¾ä¼šå¸¦ç»™ä½ \n",
    "\n",
    "1. äº†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ\n",
    "2. æŒæ¡æ¨¡å‹å¾®è°ƒ/å°å‚æ•°é‡å¾®è°ƒçš„æ“ä½œè¿‡ç¨‹\n",
    "3. æŒæ¡æ¨¡å‹å¾®è°ƒ/å°å‚æ•°é‡å¾®è°ƒå…³é”®ã€Œè¶…å‚ã€\n",
    "4. æŒæ¡è®­ç»ƒæ•°æ®çš„é€‰æ‹©ã€å‡†å¤‡ã€æ¸…æ´—ç­‰æ–¹æ³•ä¸æ€è·¯\n",
    "5. è®­ç»ƒä¸€ä¸ªå‚ç›´é¢†åŸŸçš„å¤§æ¨¡å‹\n",
    "\n",
    "å¼€å§‹ä¸Šè¯¾ï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## è¿˜æ˜¯å†™åœ¨å‰é¢\n",
    "\n",
    "1. è¿™å ‚è¯¾å†…å®¹æœ‰éš¾åº¦ï¼š\n",
    "   1. æœ‰å¾ˆå¤šé™Œç”Ÿçš„åè¯ï¼ŒåŒ…æ‹¬æ•°å­¦åè¯å’Œæ¨¡å‹ç®—æ³•æœ¬èº«çš„åè¯\n",
    "   2. æ¶‰åŠåˆ°å¾ˆå¤šæ•°å­¦çŸ¥è¯†ï¼Œå¾ˆå¤šä¸œè¥¿æœ¬èº«æ˜¯ä»æ•°å­¦æ¨å¯¼å‡ºæ¥çš„ï¼Œä¸å¥½å…·è±¡åŒ–\n",
    "   3. æ·±åº¦å­¦ä¹ é‡Œæœ‰å¤§é‡åŸºäºç»éªŒçš„æ€»ç»“ï¼Œä½“ç°æˆå„ç§è¶…å‚å’Œ Tricks\n",
    "2. è¿™å ‚è¯¾è¯¥æ€ä¹ˆå­¦ï¼š\n",
    "   1. æ³¨æ„åŠ›é›†ä¸­ï¼Œè·Ÿä¸Šæˆ‘çš„æ€è·¯\n",
    "   2. é‡åˆ°ä¸æ‡‚çš„åœ°æ–¹ï¼Œåˆ«å®³æ€•ï¼Œå…ˆå°è¯•æ€è€ƒ\n",
    "   3. å®ç°æƒ³ä¸æ˜ç™½ä¹Ÿåˆ«ç°å¿ƒï¼Œè¿™ä¸ªé¢†åŸŸçš„èƒ½åŠ›ç§¯ç´¯æ˜¯éœ€è¦æ—¶é—´çš„\n",
    "   4. çœŸæ„Ÿå…´è¶£çš„åŒå­¦ï¼Œå°è¯•å¤šåº¦è®ºæ–‡ï¼Œâ€œä¹¦è¯»ç™¾éå…¶ä¹‰è‡ªè§â€çš„é“ç†æˆ‘äº²è‡ªéªŒè¯è¿‡\n",
    "3. è‡ªå·±æ€è€ƒå¾ˆé‡è¦ï¼å›æƒ³â€œç¨‹åºå‘˜æ€ç»´å‘ç®—æ³•å·¥ç¨‹å¸ˆæ€ç»´â€çš„è½¬å˜\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning æœ‰ä»€ä¹ˆç”¨ï¼šå…ˆçœ‹ä¸€ä¸ªä¾‹å­\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://u202774-9847-a4dc9da1.westb.seetacloud.com:8443/\n",
    "\n",
    "**æ³•å¾‹é¢†åŸŸçš„é˜…è¯»ç†è§£**\n",
    "\n",
    "åˆ¤å†³ä¹¦:\n",
    "\n",
    "ç»å®¡ç†æŸ¥æ˜:2004 å¹´ 1 æœˆ 6 æ—¥,å®‰å±…ç‰©ä¸šä¸ä¸œè‡³å¿æ˜Œç››æˆ¿åœ°äº§å¼€å‘æœ‰é™è´£ä»»å…¬å¸ç­¾è®¢å‰æœŸç‰©ä¸šç®¡ç†æœåŠ¡åˆåŒ,åˆåŒçº¦å®šç”±å®‰å±…ç‰©ä¸šå¯¹ä¸œè‡³å¿æ˜Œç››æˆ¿åœ°äº§å¼€å‘æœ‰é™è´£ä»»å…¬å¸å¼€å‘çš„é£Ÿå“å°åŒºæä¾›ç‰©ä¸šæœåŠ¡,æœåŠ¡æœŸé™ä¸ºè¯¥å°åŒºä¸šä¸»å¤§ä¼šæˆç«‹æ—¶æ­¢,è¯¥åˆåŒå¯¹å§”æ‰˜ç®¡ç†äº‹é¡¹ã€åŒæ–¹çš„æƒåˆ©ä¹‰åŠ¡ã€ç‰©ä¸šç®¡ç†æœåŠ¡è¦æ±‚æ ‡å‡†ã€ç‰©ä¸šç®¡ç†æœåŠ¡è´¹ç”¨ã€è¿çº¦è´£ä»»ç­‰è¿›è¡Œäº†å…·ä½“çš„çº¦å®š 2005 å¹´ 8 æœˆ 28 æ—¥,æ±ª x3 å…¥ä½è¯¥å°åŒº 8 æ ‹ä¸€å•å…ƒ 102 å®¤,å¹¶ä¸å®‰å±…ç‰©ä¸šç­¾è®¢äº†æˆ¿å±‹å…¥ä½åè®®,çº¦å®šç‰©ä¸šç®¡ç†è´¹ä¸º 252 å…ƒ/å¹´,å¹¶æ˜ç¡®è‹¥æ±ª x3 æ— æ•…ä¸äº¤è§„å®šåº”äº¤è´¹ç”¨çš„,å®‰å±…ç‰©ä¸šå¯è¦æ±‚å…¶é™æœŸç¼´çº³å¹¶æ”¶å–åº”ç¼´è´¹ç”¨ 3%çš„æ»çº³é‡‘æ±ª x3 è‡ª 2008 å¹´ 8 æœˆ 28 æ—¥ä»¥æ¥æœªäº¤çº³ç‰©ä¸šæœåŠ¡è´¹,2013 å¹´ 12 æœˆ,å®‰å±…ç‰©ä¸šå‘æ±ª x3 ä¸‹è¾¾å‚¬äº¤ç‰©ä¸šæœåŠ¡è´¹é€šçŸ¥ä¹¦ç°å®‰å±…ç‰©ä¸šä»¥è¢«å‘Šè‡ª 2008 å¹´ 8 æœˆ 28 æ—¥èµ·è‡³ 2015 å¹´ 4 æœˆ 27 æ—¥æ­¢çš„ç‰©ä¸šæœåŠ¡è´¹ 1680 å…ƒåŠè¿çº¦é‡‘ 50.4 å…ƒæœªäº¤ä¸ºç”±è¯‰è‡³æœ¬é™¢,è¢«å‘Šåˆ™ä»¥åŸå‘Šä¸ä½œä¸ºä¸ºç”±æ‹’ä¸ç¼´çº³,ä¸ºæ­¤æˆè®¼å¦æŸ¥æ˜,æœ¬æ¡ˆæ‰€æ¶‰é£Ÿå“å°åŒºç›®å‰æœªé€‰ä¸¾ä¸šä¸»å§”å‘˜ä¼šä»¥ä¸Šäº‹å®æœ‰å½“äº‹äººé™ˆè¿°ã€å‰æœŸç‰©ä¸šç®¡ç†æœåŠ¡åˆåŒã€æˆ¿å±‹å…¥ä½åè®®ç­‰è¯æ®åœ¨å·ä½è¯,è¶³ä»¥è®¤å®š\n",
    "\n",
    "é—®é¢˜ 1: å®‰å±…ç‰©ä¸šç»™å“ªä¸ªå°åŒºåšç‰©ä¸šï¼Ÿ\n",
    "\n",
    "é—®é¢˜ 2: è¢«å‘Šæ˜¯å¦å¦‚çº¦æ”¯ä»˜ç‰©ä¸šè´¹äº†ï¼Ÿ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## æˆ‘æœ‰å¾ˆå¤šé—®é¢˜\n",
    "\n",
    "<br/>\n",
    "<img src=\"confused.png\" style=\"margin-left: 0px\" width=\"600px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸€ã€ä»€ä¹ˆæ˜¯ï¼š\n",
    "\n",
    "- æ¨¡å‹è®­ç»ƒï¼ˆTrainingï¼‰\n",
    "- é¢„è®­ç»ƒï¼ˆPre-Trainingï¼‰\n",
    "- å¾®è°ƒï¼ˆFine-Tuningï¼‰\n",
    "- è½»é‡åŒ–å¾®è°ƒï¼ˆParameter Efficient Fine-Tuning, PEFTï¼‰\n",
    "\n",
    "<br/>\n",
    "<img src=\"training.png\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## äºŒã€ä»€ä¹ˆæ˜¯æ¨¡å‹\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>å°è¯•ï¼š</b> ç”¨ç®€å•çš„æ•°å­¦è¯­è¨€è¡¨è¾¾æ¦‚å¿µ\n",
    "</div>\n",
    "\n",
    "### 2.1ã€é€šä¿—ï¼ˆä¸ä¸¥è°¨ï¼‰çš„è¯´ã€æ¨¡å‹æ˜¯ä¸€ä¸ªå‡½æ•°ï¼š\n",
    "\n",
    "$y=F(x;\\omega)$\n",
    "\n",
    "- å®ƒæ¥æ”¶è¾“å…¥$x$ï¼šå¯ä»¥æ˜¯ä¸€ä¸ªè¯ã€ä¸€ä¸ªå¥å­ã€ä¸€ç¯‡æ–‡ç« æˆ–å›¾ç‰‡ã€è¯­éŸ³ã€è§†é¢‘ ...\n",
    "  - è¿™äº›ç‰©ä½“éƒ½è¢«è¡¨ç¤ºæˆä¸€ä¸ªæ•°å­¦ã€ŒçŸ©é˜µã€ï¼ˆå…¶å®åº”è¯¥å«å¼ é‡ï¼Œtensorï¼‰\n",
    "- å®ƒé¢„æµ‹è¾“å‡º$y$\n",
    "  - å¯ä»¥æ˜¯ã€Œæ˜¯å¦ã€ï¼ˆ{0,1}ï¼‰ã€æ ‡ç­¾ï¼ˆ{0,1,2,3...}ï¼‰ã€ä¸€ä¸ªæ•°å€¼ï¼ˆå›å½’é—®é¢˜ï¼‰ã€ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡ ...\n",
    "- å®ƒçš„è¡¨è¾¾å¼å°±æ˜¯ç½‘ç»œç»“æ„ï¼ˆè¿™é‡Œç‰¹æŒ‡æ·±åº¦å­¦ä¹ ï¼‰\n",
    "- å®ƒæœ‰ä¸€ç»„å‚æ•°$\\omega$ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¦è®­ç»ƒçš„éƒ¨åˆ†\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªæ–¹ç¨‹ï¼š</b> \n",
    "    <ol>\n",
    "        <li>æ¯æ¡æ•°æ®å°±æ˜¯ä¸€å¯¹å„¿ $(x,y)$ ï¼Œå®ƒä»¬æ˜¯å¸¸é‡</li>\n",
    "        <li>å‚æ•°æ˜¯æœªçŸ¥æ•°ï¼Œæ˜¯å˜é‡</li>\n",
    "        <li>$F$ å°±æ˜¯è¡¨è¾¾å¼ï¼šæˆ‘ä»¬ä¸çŸ¥é“çœŸå®çš„å…¬å¼æ˜¯ä»€ä¹ˆæ ·çš„ï¼Œæ‰€ä»¥å‡è®¾äº†ä¸€ä¸ªè¶³å¤Ÿå¤æ‚çš„å…¬å¼ï¼ˆæ¯”å¦‚ï¼Œä¸€ä¸ªç‰¹å®šç»“æ„çš„ç¥ç»ç½‘ç»œï¼‰</li>\n",
    "        <li>è¿™ä¸ªæ±‚è§£è¿™ä¸ªæ–¹ç¨‹ï¼ˆè¿‘ä¼¼è§£ï¼‰å°±æ˜¯è®­ç»ƒè¿‡ç¨‹</li>\n",
    "    </ol>\n",
    "</div>\n",
    "\n",
    "### 2.2ã€ä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ\n",
    "\n",
    "ä¸€ä¸ªç¥ç»å…ƒï¼š$y=f(\\sum_i w_i\\cdot x_i)$\n",
    "\n",
    "<img src=\"neuron.jpg\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "æŠŠå¾ˆå¤šç¥ç»å…ƒè¿æ¥èµ·æ¥ï¼Œå°±æˆäº†ç¥ç»ç½‘ç»œï¼š$y=f(\\sum_i w_i\\cdot x_i)$ã€$z=f(\\sum_i w'_i\\cdot y_i)$ã€$\\tau=f(\\sum_i w''_i\\cdot z_i)$ã€...\n",
    "\n",
    "<img src=\"network.jpg\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "è¿™é‡Œçš„$f$å«æ¿€æ´»å‡½æ•°ï¼Œæœ‰å¾ˆå¤šç§å½¢å¼\n",
    "\n",
    "<img src=\"activation.jpeg\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b> è¿™é‡Œå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ä¼šæ€æ ·ï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸‰ã€ä»€ä¹ˆæ˜¯æ¨¡å‹è®­ç»ƒ\n",
    "\n",
    "æˆ‘ä»¬å¸Œæœ›æ‰¾åˆ°ä¸€ç»„å‚æ•°$\\omega$ï¼Œä½¿æ¨¡å‹é¢„æµ‹çš„è¾“å‡º$\\hat{y}=F(x;\\omega)$ä¸çœŸå®çš„è¾“å‡º$y$ï¼Œå°½å¯èƒ½çš„æ¥è¿‘\n",
    "\n",
    "è¿™é‡Œï¼Œæˆ‘ä»¬ï¼ˆè‡³å°‘ï¼‰éœ€è¦ä¸¤ä¸ªè¦ç´ ï¼š\n",
    "\n",
    "- ä¸€ä¸ªæ•°æ®é›†ï¼ŒåŒ…å«$N$ä¸ªè¾“å…¥è¾“å‡ºçš„ä¾‹å­ï¼ˆç§°ä¸ºæ ·æœ¬ï¼‰ï¼š$D=\\{(x_i,y_i)\\}_{i=1}^N$\n",
    "- ä¸€ä¸ªæŸå¤±å‡½æ•°ï¼Œè¡¡é‡æ¨¡å‹é¢„æµ‹çš„è¾“å‡ºä¸çœŸå®è¾“å‡ºä¹‹é—´çš„å·®è·ï¼š$\\mathrm{loss}(y,F(x;\\omega))$\n",
    "\n",
    "### 3.1ã€æ¨¡å‹è®­ç»ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªæ±‚è§£æœ€ä¼˜åŒ–é—®é¢˜çš„è¿‡ç¨‹\n",
    "\n",
    "$\\min_{\\omega} L(D,\\omega)$\n",
    "\n",
    "$L(D,\\omega)=\\frac{1}{N}\\sum_{i=1}^N\\mathrm{loss}(y,F(x;\\omega))$\n",
    "\n",
    "### 3.2ã€æ€ä¹ˆæ±‚è§£\n",
    "\n",
    "å›å¿†ä¸€ä¸‹æ¢¯åº¦çš„å®šä¹‰\n",
    "\n",
    "<img src=\"gradient.svg\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "ä»æœ€ç®€å•çš„æƒ…å†µè¯´èµ·ï¼šæ¢¯åº¦ä¸‹é™ä¸å‡¸é—®é¢˜\n",
    "\n",
    "<img src=\"gradient.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "æ¢¯åº¦å†³å®šäº†å‡½æ•°å˜åŒ–çš„æ–¹å‘ï¼Œæ¯æ¬¡è¿­ä»£æ›´æ–°æˆ‘ä»¬ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªæå€¼\n",
    "\n",
    "$\\omega_{n+1}\\leftarrow \\omega_n - \\gamma \\nabla_{\\omega}L(D,\\omega)$\n",
    "\n",
    "å…¶ä¸­ï¼Œ$\\gamma<1$å«åšå­¦ä¹ ç‡ï¼Œå®ƒå’Œæ¢¯åº¦çš„æ¨¡æ•°å…±åŒå†³å®šäº†æ¯æ­¥èµ°å¤šè¿œ\n",
    "\n",
    "### 3.3ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ1ï¼‰**ï¼šåœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šæ±‚æ¢¯åº¦ï¼Œè®¡ç®—é‡å¤ªå¤§äº†\n",
    "\n",
    "<br/>\n",
    "<img src=\"batch.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>æ¡ä»¶å…è®¸çš„æƒ…å†µä¸‹ï¼ŒBatch Sizeå°½é‡å¤§äº›\n",
    "</div>\n",
    "\n",
    "### 3.4ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ2ï¼‰**ï¼šæ·±åº¦å­¦ä¹ æ²¡æœ‰å…¨å±€æœ€ä¼˜è§£ï¼ˆéå‡¸é—®é¢˜ï¼‰\n",
    "\n",
    "<br/>\n",
    "<img src=\"local_minima.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "### 3.5ã€**ç°å®æ€»æ˜¯æ²¡é‚£ä¹ˆç®€å•ï¼ˆ3ï¼‰**ï¼šå­¦ä¹ ç‡ä¹Ÿå¾ˆå…³é”®ï¼Œç”šè‡³éœ€è¦åŠ¨æ€è°ƒæ•´\n",
    "\n",
    "<br/>\n",
    "<img src=\"lr.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>åˆ’é‡ç‚¹ï¼š</b>é€‚å½“è°ƒæ•´å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰ï¼Œé¿å…é™·å…¥å¾ˆå·®çš„å±€éƒ¨è§£æˆ–è€…è·³è¿‡äº†å¥½çš„è§£\n",
    "</div>\n",
    "\n",
    "### 3.6ã€ç¥ç»ç½‘ç»œçš„æ¢¯åº¦æ€ä¹ˆæ±‚ï¼ˆé€‰å­¦ï¼‰\n",
    "\n",
    "Chain Rule:\n",
    "\n",
    "å‡è®¾ $L(w)=f(g(h(w)))$\n",
    "\n",
    "é‚£ä¹ˆ $L'(w)=f'(g(h(w))) \\cdot g'(h(w)) \\cdot h'(w)$\n",
    "\n",
    "<img src=\"backprop.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "è“è‰²çš„è¿‡ç¨‹å« Forward Passï¼Œçº¢è‰²çš„è¿‡ç¨‹å« Backward Passï¼Œæ•´ä¸ªè¿‡ç¨‹å« Backpropagation\n",
    "\n",
    "## å››ã€æ±‚è§£å™¨\n",
    "\n",
    "ä¸ºäº†è®©è®­ç»ƒè¿‡ç¨‹æ›´å¥½çš„æ”¶æ•›ï¼Œäººä»¬è®¾è®¡äº†å¾ˆå¤šæ›´å¤æ‚çš„æ±‚è§£å™¨\n",
    "\n",
    "- æ¯”å¦‚ï¼šSGDã€L-BFGSã€Rpropã€RMSpropã€Adamã€AdamWã€AdaGradã€AdaDelta ç­‰ç­‰\n",
    "- ä½†æ˜¯ï¼Œå¥½åœ¨æœ€å¸¸ç”¨çš„å°±æ˜¯ Adam æˆ–è€… AdamW\n",
    "\n",
    "## äº”ã€ä¸€äº›å¸¸ç”¨çš„æŸå¤±å‡½æ•°\n",
    "\n",
    "- ä¸¤ä¸ªæ•°å€¼çš„å·®è·ï¼ŒMin Square Errorï¼š$\\ell_{\\mathrm{MSE}}=\\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2$ (ç­‰ä»·äºæ¬§å¼è·ç¦»ï¼Œè§ä¸‹æ–‡)\n",
    "- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„ï¼ˆæ¬§å¼ï¼‰è·ç¦»ï¼š$\\ell(\\mathbf{y},\\mathbf{\\hat{y}})=\\|\\mathbf{y}-\\mathbf{\\hat{y}}\\|$\n",
    "- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„å¤¹è§’ï¼ˆä½™å¼¦è·ç¦»ï¼‰ï¼š\n",
    "  <img src=\"cosine_loss.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "- ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ï¼Œäº¤å‰ç†µï¼š$\\ell_{\\mathrm{CE}}(p,q)=-\\sum_i p_i\\log q_i$ â€”â€”å‡è®¾æ˜¯æ¦‚ç‡åˆ†å¸ƒ p,q æ˜¯ç¦»æ•£çš„\n",
    "- è¿™äº›æŸå¤±å‡½æ•°ä¹Ÿå¯ä»¥ç»„åˆä½¿ç”¨ï¼ˆåœ¨æ¨¡å‹è’¸é¦çš„åœºæ™¯å¸¸è§è¿™ç§æƒ…å†µï¼‰ï¼Œä¾‹å¦‚$L=L_1+\\lambda L_2$ï¼Œå…¶ä¸­$\\lambda$æ˜¯ä¸€ä¸ªé¢„å…ˆå®šä¹‰çš„æƒé‡ï¼Œä¹Ÿå«ä¸€ä¸ªã€Œè¶…å‚ã€\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b> ä½ èƒ½æ‰¾åˆ°è¿™äº›æŸå¤±å‡½æ•°å’Œåˆ†ç±»ã€èšç±»ã€å›å½’é—®é¢˜ä¹‹é—´çš„å…³ç³»å—ï¼Ÿ\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…­ã€ç”¨ PyTorch è®­ç»ƒä¸€ä¸ªæœ€ç®€å•çš„ç¥ç»ç½‘ç»œ\n",
    "\n",
    "æ•°æ®é›†ï¼ˆMNISTï¼‰æ ·ä¾‹ï¼š\n",
    "\n",
    "<img src=\"MNIST.jpg\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "è¾“å…¥ä¸€å¼  28Ã—28 çš„å›¾åƒï¼Œè¾“å‡ºæ ‡ç­¾ 0--9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "TEST_BACTH_SIZE = 1000\n",
    "EPOCHS = 5\n",
    "LR = 0.01\n",
    "GAMMA = 0.9\n",
    "WEIGHT_DECAY = 1e-6\n",
    "SEED = 42\n",
    "LOG_INTERVAL = 100\n",
    "\n",
    "# å®šä¹‰ä¸€ä¸ªå…¨è¿æ¥ç½‘ç»œ\n",
    "\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ç¬¬ä¸€å±‚784ç»´è¾“å…¥ã€256ç»´è¾“å‡º -- å›¾åƒå¤§å°28Ã—28=784\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        # ç¬¬äºŒå±‚256ç»´è¾“å…¥ã€128ç»´è¾“å‡º\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        # ç¬¬ä¸‰å±‚128ç»´è¾“å…¥ã€64ç»´è¾“å‡º\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        # ç¬¬ä¸‰å±‚64ç»´è¾“å…¥ã€10ç»´è¾“å‡º -- è¾“å‡ºç±»åˆ«10ç±»ï¼ˆ0,1,...9ï¼‰\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "        # Dropout module with 0.2 drop probability\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # æŠŠè¾“å…¥å±•å¹³æˆ1Då‘é‡\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # æ¯å±‚æ¿€æ´»å‡½æ•°æ˜¯ReLUï¼Œé¢å¤–åŠ dropout\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "\n",
    "        # è¾“å‡ºä¸º10ç»´æ¦‚ç‡åˆ†å¸ƒ\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "# è®­ç»ƒè¿‡ç¨‹\n",
    "\n",
    "\n",
    "def train(model, loss_fn, device, train_loader, optimizer, epoch):\n",
    "    # å¼€å¯æ¢¯åº¦è®¡ç®—\n",
    "    model.train()\n",
    "    for batch_idx, (data_input, true_label) in enumerate(train_loader):\n",
    "        # ä»æ•°æ®åŠ è½½å™¨è¯»å–ä¸€ä¸ªbatch\n",
    "        # æŠŠæ•°æ®ä¸Šè½½åˆ°GPUï¼ˆå¦‚æœ‰ï¼‰\n",
    "        data_input, true_label = data_input.to(device), true_label.to(device)\n",
    "        # æ±‚è§£å™¨åˆå§‹åŒ–ï¼ˆæ¯ä¸ªbatchåˆå§‹åŒ–ä¸€æ¬¡ï¼‰\n",
    "        optimizer.zero_grad()\n",
    "        # æ­£å‘ä¼ æ’­ï¼šæ¨¡å‹ç”±è¾“å…¥é¢„æµ‹è¾“å‡º\n",
    "        output = model(data_input)\n",
    "        # è®¡ç®—loss\n",
    "        loss = loss_fn(output, true_label)  # F.nll_loss(output, target)\n",
    "        # åå‘ä¼ æ’­ï¼šè®¡ç®—å½“å‰batchçš„lossçš„æ¢¯åº¦\n",
    "        loss.backward()\n",
    "        # ç”±æ±‚è§£å™¨æ ¹æ®æ¢¯åº¦æ›´æ–°æ¨¡å‹å‚æ•°\n",
    "        optimizer.step()\n",
    "\n",
    "        # é—´éš”æ€§çš„è¾“å‡ºå½“å‰batchçš„è®­ç»ƒloss\n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data_input), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "# è®¡ç®—åœ¨æµ‹è¯•é›†çš„å‡†ç¡®ç‡å’Œloss\n",
    "def test(model, loss_fn, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # sum up batch loss\n",
    "            test_loss += loss_fn(output, target, reduction='sum').item()\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "def main():\n",
    "    # æ£€æŸ¥æ˜¯å¦æœ‰GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "\n",
    "    # è®¾ç½®éšæœºç§å­ï¼ˆä»¥ä¿è¯ç»“æœå¯å¤ç°ï¼‰\n",
    "    torch.manual_seed(SEED)\n",
    "\n",
    "    # è®­ç»ƒè®¾å¤‡ï¼ˆGPUæˆ–CPUï¼‰\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # è®¾ç½®batch size\n",
    "    train_kwargs = {'batch_size': BATCH_SIZE}\n",
    "    test_kwargs = {'batch_size': TEST_BACTH_SIZE}\n",
    "\n",
    "    if use_cuda:\n",
    "        cuda_kwargs = {'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True}\n",
    "        train_kwargs.update(cuda_kwargs)\n",
    "        test_kwargs.update(cuda_kwargs)\n",
    "\n",
    "    # æ•°æ®é¢„å¤„ç†ï¼ˆè½¬tensorã€æ•°å€¼å½’ä¸€åŒ–ï¼‰\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # è‡ªåŠ¨ä¸‹è½½MNISTæ•°æ®é›†\n",
    "    dataset_train = datasets.MNIST('data', train=True, download=True,\n",
    "                                   transform=transform)\n",
    "    dataset_test = datasets.MNIST('data', train=False,\n",
    "                                  transform=transform)\n",
    "\n",
    "    # å®šä¹‰æ•°æ®åŠ è½½å™¨ï¼ˆè‡ªåŠ¨å¯¹æ•°æ®åŠ è½½ã€å¤šçº¿ç¨‹ã€éšæœºåŒ–ã€åˆ’åˆ†batchã€ç­‰ç­‰ï¼‰\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, **train_kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(dataset_test, **test_kwargs)\n",
    "\n",
    "    # åˆ›å»ºç¥ç»ç½‘ç»œæ¨¡å‹\n",
    "    model = FeedForwardNet().to(device)\n",
    "\n",
    "    # æŒ‡å®šæ±‚è§£å™¨\n",
    "    optimizer = optim.SGD(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    # scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
    "\n",
    "    # å®šä¹‰losså‡½æ•°\n",
    "    loss_fn = F.cross_entropy\n",
    "\n",
    "    # è®­ç»ƒNä¸ªepoch\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train(model, loss_fn, device, train_loader, optimizer, epoch)\n",
    "        test(model, loss_fn, device, test_loader)\n",
    "        # scheduler.step()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>å¦‚ä½•è¿è¡Œè¿™æ®µä»£ç ï¼š</b>\n",
    "<ol>\n",
    "    <li>ä¸è¦åœ¨Jupyterç¬”è®°ä¸Šç›´æ¥è¿è¡Œ</li>\n",
    "    <li>è¯·å°†å·¦ä¾§çš„ exp.py æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°</li>\n",
    "    <li>å®‰è£…ç›¸å…³ä¾èµ–åŒ…: pip install torch torchvision</li>\n",
    "    <li>è¿è¡Œï¼špython3 exp.py</li>\n",
    "</ol>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¸ƒã€å…ˆä»‹ç»å‡ ä¸ªå¸¸ç”¨çš„è¶…å‚\n",
    "\n",
    "### 7.1ã€è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ\n",
    "\n",
    "<br />\n",
    "<img src=\"overfit.png\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>å¥¥å¡å§†å‰ƒåˆ€ï¼š</b> ä¸¤ä¸ªå¤„äºç«äº‰åœ°ä½çš„ç†è®ºèƒ½å¾—å‡ºåŒæ ·çš„ç»“è®ºï¼Œé‚£ä¹ˆç®€å•çš„é‚£ä¸ªæ›´å¥½ã€‚\n",
    "</div>\n",
    "\n",
    "**é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼ˆ1ï¼‰ï¼š**Weight Decay\n",
    "\n",
    "$J(\\omega)=L(D,\\omega)+\\lambda\\|\\omega\\| \\Rightarrow \\nabla_{\\omega}J=\\nabla_{\\omega}L + \\frac{1}{2}\\lambda\\omega$\n",
    "\n",
    "- æƒ©ç½šå‚æ•°çš„å¤æ‚æ€§ï¼ˆ$L_2$-normï¼‰ï¼šç­‰ä»·ä¸åœ¨æ¢¯åº¦ä¸Šå‡å»å‚æ•°æœ¬èº«ï¼ˆä¹˜ä¸€ä¸ªå°æ•°ä½œä¸ºæƒé‡ï¼‰\n",
    "- Weight Decay å°±æ˜¯å‰é¢é‚£ä¸ªæƒé‡$\\lambda$\n",
    "\n",
    "**é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼ˆ2ï¼‰ï¼š**Dropout\n",
    "\n",
    "- æˆ‘ä»¬åœ¨å‰å‘ä¼ æ’­çš„æ—¶å€™ï¼Œæ¦‚ç‡æ€§çš„ï¼ˆä¸´æ—¶ï¼‰åˆ é™¤ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼Œè¿™æ ·å¯ä»¥ä½¿æ¨¡å‹æ³›åŒ–æ€§æ›´å¼ºï¼Œå› ä¸ºå®ƒä¸ä¼šå¤ªä¾èµ–æŸäº›å±€éƒ¨çš„ç‰¹å¾\n",
    "- è¿™æ ·è®­ç»ƒ$N$æ¬¡ï¼Œç­‰ä»·äºè®­ç»ƒ$N$ä¸åŒçš„ç½‘ç»œï¼Œå†å–å¹³å‡å€¼ï¼›$N$ä¸ªç½‘ç»œä¸ä¼šåŒæ—¶è¿‡æ‹Ÿåˆäºä¸ä¸€ä¸ªç»“æœï¼Œè¿™æ ·å¹³å‡å€¼çš„æ–¹å¼èƒ½æœ‰æ•ˆå‡å°‘è¿‡æ‹Ÿåˆçš„å¹²æ‰°ã€‚\n",
    "\n",
    "<img src=\"dropout.jfif\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "### 7.2ã€å­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥\n",
    "\n",
    "- å¼€å§‹æ—¶å­¦ä¹ ç‡å¤§äº›ï¼šå¿«é€Ÿåˆ°è¾¾æœ€ä¼˜è§£é™„è¿‘\n",
    "- é€æ¸å‡å°å­¦ä¹ ç‡ï¼šé¿å…è·³è¿‡æœ€ä¼˜è§£\n",
    "- NLP ä»»åŠ¡çš„æŸå¤±å‡½æ•°æœ‰å¾ˆå¤šâ€œæ‚¬å´–å³­å£â€ï¼Œè‡ªé€‚åº”å­¦ä¹ ç‡æ›´èƒ½å¤„ç†è¿™ç§æç«¯æƒ…å†µï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ã€‚\n",
    "\n",
    "<img src=\"scheduler.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "å‡ ç§å¸¸ç”¨çš„å­¦ä¹ ç‡è°ƒæ•´å™¨\n",
    "\n",
    "<img src=\"lr_scheduler.jpg\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "**é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ–¹æ³•ï¼ˆ3ï¼‰ï¼š**å­¦ä¹ ç‡ Warm Up\n",
    "\n",
    "å…ˆä»ä¸€ä¸ªå¾ˆå°çš„å­¦ä¹ ç‡é€æ¸ä¸Šå‡åˆ°æ­£å¸¸å­¦ä¹ ç‡ï¼Œåœ¨ç¨³æ­¥å‡å°å­¦ä¹ ç‡\n",
    "\n",
    "- å…¶åŸç†å°šæœªè¢«å……åˆ†è¯æ˜\n",
    "- ç»éªŒä¸»ä¹‰è§£é‡Šï¼šå‡ç¼“æ¨¡å‹åœ¨åˆå§‹é˜¶æ®µå¯¹ mini-batch çš„æå‰è¿‡æ‹Ÿåˆç°è±¡ï¼Œä¿æŒåˆ†å¸ƒçš„å¹³ç¨³\n",
    "- ç»éªŒä¸»ä¹‰è§£é‡Šï¼šæœ‰åŠ©äºä¿æŒæ¨¡å‹æ·±å±‚çš„ç¨³å®šæ€§\n",
    "\n",
    "<img src=\"warmup.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>åº”ç”¨åœºæ™¯ï¼š</b> (1) å½“ç½‘ç»œéå¸¸å®¹æ˜“nanæ—¶å€™ï¼›(2) å¦‚æœè®­ç»ƒé›†æŸå¤±å¾ˆä½ï¼Œå‡†ç¡®ç‡é«˜ï¼Œä½†æµ‹è¯•é›†æŸå¤±å¤§ï¼Œå‡†ç¡®ç‡ä½. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å…«ã€è‡ªç„¶è¯­è¨€å¤„ç†å¸¸è§çš„ç½‘ç»œç»“æ„\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b> å›¾åƒå¤©ç”Ÿå¯ä»¥è¡¨ç¤ºæˆçŸ©é˜µï¼ˆæˆ–tensorï¼‰ï¼Œé‚£æ–‡æœ¬æ€ä¹ˆè¡¨ç¤ºæˆçŸ©é˜µï¼ˆæˆ–tensorï¼‰\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1ã€æ–‡æœ¬å·ç§¯ç¥ç»ç½‘ç»œ TextCNN\n",
    "\n",
    "<br />\n",
    "\n",
    "ä¸€ä¸ªçª—å£çš„å·ç§¯å’ŒPoolingè¿‡ç¨‹\n",
    "\n",
    "<img src=\"conv_maxpooling_steps.gif\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "ä¸åŒå¤§å°çš„çª—å£åˆ†åˆ«åšå·ç§¯å’ŒPoolingï¼Œç»“æœæ‹¼åœ¨ä¸€èµ·\n",
    "\n",
    "<img src=\"TextCNN.jpg\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "- å‚æ•°é‡è¾ƒå°‘ã€å¥½è®­ç»ƒã€ç®—åŠ›è¦æ±‚ä½\n",
    "- é€‚åˆæ–‡æœ¬åˆ†ç±»é—®é¢˜\n",
    "- å–„äºè¡¨ç¤ºå±€éƒ¨ç‰¹å¾ï¼ˆå·ç§¯çª—å£ï¼‰ï¼Œä¸æ“…é•¿è¡¨ç¤ºé•¿ä¸Šä¸‹æ–‡ä¾èµ–å…³ç³»\n",
    "\n",
    "### 8.2ã€å¾ªç¯ç¥ç»ç½‘ç»œ RNN\n",
    "\n",
    "<br />\n",
    "\n",
    "é¦–å…ˆï¼šè¾“å…¥æ˜¯ä¸€ä¸ªåºåˆ—\n",
    "\n",
    "<img src=\"RNN.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "ä½†è¿™ç§ç®€æ˜“ RNN æœ‰å¾ˆå¤šé—®é¢˜ï¼Œæœ€å¤§é—®é¢˜æ˜¯éšç€åºåˆ—é•¿åº¦å¢åŠ ï¼Œæ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸\n",
    "\n",
    "<img src=\"LSTM.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "LSTM å’Œ GRU é€šè¿‡ã€Œé—¨ã€æ¥æ§åˆ¶ä¸Šæ–‡çš„çŠ¶æ€è¢«è®°ä½è¿˜æ˜¯é—å¿˜ï¼ŒåŒæ—¶é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸\n",
    "\n",
    "ä»¥LSTMä¸ºä¾‹ï¼š\n",
    "\n",
    "<img src=\"lstm.jfif\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 8.3ã€Attention (for RNN)\n",
    "\n",
    "<br />\n",
    "\n",
    "ç»™å®šå½“å‰çš„è¾“å…¥ï¼Œä¸Šæ–‡çš„ä¸€äº›ä¿¡æ¯æ¯”å¦ä¸€äº›é‡è¦\n",
    "\n",
    "<img src=\"attention.gif\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "<br />\n",
    "\n",
    "<font color='blue'>äºæ˜¯è®¾è®¡ä¸€ä¸ªå¯å¾®çš„å‡½æ•°å°±å¯ä»¥æŠŠå®ƒåŠ å…¥åˆ°ç½‘ç»œä¸­æ¥è¯•è¯•ï¼Œåæ­£ä¹Ÿæ²¡æœ‰å…¨å±€æœ€ä¼˜è§£</font>\n",
    "\n",
    "<br />\n",
    "<img src=\"attention-fn.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "- å¯¹å½“å‰ token $t$çš„ä¸Šæ–‡çš„æ¯ä¸ª token $i$è®¡ç®—ä¸Šè¿° score\n",
    "- å°†è¿™äº› score åš softmax å¾—åˆ°æƒé‡$\\alpha_{t,i}$\n",
    "- å°†ä¸Šæ–‡çš„éšå±‚çŠ¶æ€ä¹˜ä»¥å…¶æƒé‡å¹¶ç›¸åŠ $c_t=\\sum_i\\alpha_{t,i}h_i$\n",
    "- å°†$c_t$ä¸å½“å‰ token çš„çŠ¶æ€æ‹¼æ¥åœ¨ä¸€èµ·$s_t=\\mathrm{concat}(h_t,c_t)$\n",
    "- æ¿€æ´»è¾“å‡º$y_t=\\sigma(s_t)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¹ã€Transformer æ±Ÿå±±ä¸€ç»Ÿ\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>æ€è€ƒï¼š</b> RNNæœ‰ä»€ä¹ˆç¼ºç‚¹ï¼Ÿå¤§æ¨¡å‹ä¸ºä»€ä¹ˆä¸æ˜¯å¾ˆå¤šå±‚RNNï¼Ÿ\n",
    "</div>\n",
    "\n",
    "<br />\n",
    "<img src=\"transformer.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "### 9.1ã€**æ¶ˆé™¤ææƒ§ï¼š**æˆ‘ä»¬äº²æ‰‹å†™ä¸€ä¸ª Transformer\n",
    "\n",
    "### 9.1.1ã€Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, embed_size).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, embed_size, 2).float()\n",
    "                    * -(math.log(10000.0) / embed_size)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: è¯è¡¨å¤§å°\n",
    "        :param embed_size: embeddingç»´åº¦768\n",
    "        :param dropout: dropoutæ¦‚ç‡\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(\n",
    "            vocab_size, embed_size, padding_idx=0)\n",
    "        self.position_embedding = PositionalEmbedding(\n",
    "            embed_size=embed_size, max_len=512)\n",
    "        self.token_type_embedding = nn.Embedding(2, embed_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(\n",
    "            input_ids) + self.token_type_embedding(token_type_ids)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.2ã€å•å¤´ Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "import math\n",
    "\n",
    "'''\n",
    "query = query_linear(x)\n",
    "key = key_linear(x)\n",
    "value = value_linear(x)\n",
    "'''\n",
    "\n",
    "# å•ä¸ªå¤´çš„æ³¨æ„åŠ›è®¡ç®—\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "            / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "æ¯ä¸ªtokenå¯¹åº”çš„queryå‘é‡ä¸æ¯ä¸ªtokenå¯¹åº”çš„keyå‘é‡åšå†…ç§¯\n",
    "\n",
    "<img src=\"kq2.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "å°†ä¸Šè¿°å†…ç§¯å–softmaxï¼ˆå¾—åˆ°0~1ä¹‹é—´çš„å€¼ï¼Œå³ä¸ºattentionæƒé‡ï¼‰\n",
    "\n",
    "<img src=\"kq_softmax.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "è®¡ç®—æ¯ä¸ªtokenç›¸å¯¹äºæ‰€æœ‰å…¶å®ƒtokençš„attentionæƒé‡ï¼ˆæœ€ç»ˆæ„æˆä¸€ä¸ª$L\\times L$çš„attentionçŸ©é˜µï¼‰\n",
    "\n",
    "<img src=\"kq_softmax2.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "æ¯ä¸ªtokenå¯¹åº”çš„valueå‘é‡ä¹˜ä»¥attentionæƒé‡ï¼Œå¹¶ç›¸åŠ ï¼Œå¾—åˆ°å½“å‰tokençš„self-attention valueå‘é‡\n",
    "\n",
    "<img src=\"v.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "å°†ä¸Šè¿°æ“ä½œåº”ç”¨äºæ¯ä¸ªtoken\n",
    "<img src=\"v2.gif\" style=\"margin-left: 0px\" width=\"800px\">\n",
    "\n",
    "<br />\n",
    "ä»¥ä¸Šæ˜¯ä¸€ä¸ªå¤´çš„æ“ä½œï¼ŒåŒæ—¶ï¼ˆå¹¶è¡Œï¼‰åº”ç”¨äºå¤šä¸ªç‹¬ç«‹çš„å¤´\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.3ã€å¤šå¤´ Attention\n",
    "\n",
    "å°†æ¯ä¸ªå¤´å¾—åˆ°å‘é‡æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œæœ€åä¹˜ä¸€ä¸ªçº¿æ€§çŸ©é˜µï¼Œå¾—åˆ° multi-head attention çš„è¾“å‡º\n",
    "\n",
    "<img src=\"multi-head.gif\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, head_num, hidden_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param head_num: å¤´çš„ä¸ªæ•°ï¼Œå¿…é¡»èƒ½è¢«hidden_sizeæ•´é™¤\n",
    "        :param hidden_size: éšå±‚çš„ç»´åº¦ï¼Œä¸embed_sizeä¸€è‡´\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        assert hidden_size % head_num == 0\n",
    "\n",
    "        self.per_head_dim = hidden_size // head_num\n",
    "        self.head_num = head_num\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.query_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_linear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.output_linear = nn.Linear(hidden_size, hidden_size)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def reshape(self, x, batch_size):\n",
    "        # æ‹†æˆå¤šä¸ªå¤´\n",
    "        return x.view(batch_size, -1, self.head_num, self.per_head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        query = self.reshape(self.query_linear(x))\n",
    "        key = self.reshape(self.key_linear(x))\n",
    "        value = self.reshape(self.value_linear(x))\n",
    "\n",
    "        # æ¯ä¸ªå¤´è®¡ç®—attention\n",
    "        x, attn = self.attention(\n",
    "            query, key, value, mask=mask, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # æŠŠæ¯ä¸ªå¤´çš„attention*valueæ‹¼æ¥åœ¨ä¸€èµ·\n",
    "        x = x.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.hidden_size)\n",
    "\n",
    "        # ä¹˜ä¸€ä¸ªçº¿æ€§çŸ©é˜µ\n",
    "        return self.output_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.4ã€å…¨è¿æ¥ç½‘ç»œï¼ˆFeed-Forward Networkï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.input_layer = nn.Linear(hidden_size, hidden_size*4)\n",
    "        self.output_layer = nn.Linear(hidden_size*4, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.5ã€æ‹¼æˆä¸€å±‚ Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_size, head_num, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = MultiHeadedAttention(head_num, hidden_size)\n",
    "        self.feed_forward = FeedForward(hidden_size, dropout=dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x0 = x\n",
    "        # å¤šå¤´æ³¨æ„åŠ›å±‚\n",
    "        x = self.multi_head_attention(x, mask)\n",
    "\n",
    "        # æ®‹å·®å’ŒLayerNormå±‚(1)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_norm1(x0+x)\n",
    "\n",
    "        # å‰å‘ç½‘ç»œå±‚\n",
    "        x1 = x\n",
    "        x = self.feed_forward(x)\n",
    "\n",
    "        # æ®‹å·®å’ŒLayerNormå±‚(2)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer_norm2(x1+x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br />\n",
    "Multi-head attentionçš„è¾“å‡ºï¼Œç»è¿‡æ®‹å·®å’Œnormä¹‹åè¿›å…¥ä¸€ä¸ªä¸¤å±‚å…¨è¿æ¥ç½‘ç»œ\n",
    "<img src=\"ffn.gif\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layernorm:\n",
    "\n",
    "$y=\\frac{x-\\mathrm{E}(x)}{\\sqrt{\\mathrm{Var}(x)+\\epsilon}}*\\gamma+\\beta$\n",
    "\n",
    "å…¶ä¸­ $\\gamma$ å’Œ $\\beta$ æ˜¯å¯è®­ç»ƒçš„å‚æ•°ï¼Œ$\\epsilon=10^{-5}$æ˜¯è¶…å‚ï¼Œä¿æŒæ•°å€¼ç¨³å®šæ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.6ã€å¤šå±‚ Transformer æ„æˆ BERT Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=768, layer_num=12, head_num=12, dropout=0.1):\n",
    "\n",
    "        super().__init__()\n",
    "        # Embeddingå±‚\n",
    "        self.embedding = BERTEmbedding(\n",
    "            vocab_size=vocab_size, embed_size=hidden_size)\n",
    "        # Nå±‚Transformers\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden_size, head_num, dropout)\n",
    "             for _ in range(layer_num)]\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        \"\"\"\n",
    "        tokenizer([\"ä½ å¥½å—\",\"ä½ å¥½\"], text_pair=[\"æˆ‘å¾ˆå¥½\",\"æˆ‘å¥½\"], max_length=10, padding='max_length',truncation=True)\n",
    "        [CLS]ä½ å¥½å—[SEP]æˆ‘å¾ˆå¥½[SEP][PAD]\n",
    "        [CLS]ä½ å¥½[SEP]æˆ‘å¥½[SEP][PAD][PAD][PAD]  \n",
    "        input_ids: [\n",
    "            [101, 872, 1962, 1408, 102, 2769, 2523, 1962, 102, 0],\n",
    "            [101, 872, 1962, 102, 2769, 1962, 102, 0, 0, 0]\n",
    "        ]\n",
    "        token_type_idsï¼š[\n",
    "                [0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
    "                [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]\n",
    "            ]\n",
    "        \"\"\"\n",
    "        attention_mask = (x > 0).unsqueeze(\n",
    "            1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # è®¡ç®—embedding\n",
    "        x = self.embedding(input_ids, token_type_ids)\n",
    "\n",
    "        # é€å±‚ä»£å…¥Tranformers\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, attention_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Transformer æ€ä¹ˆç”¨\n",
    "\n",
    "### 9.2.1. Encoder-Only LM ç”¨äºæ–‡æœ¬è¡¨ç¤º\n",
    "\n",
    "é’ˆå¯¹ä¸åŒä¸‹æ¸¸ä»»åŠ¡ï¼Œåœ¨ Encoder ä¸Šé¢æ·»åŠ ä¸åŒçš„è¾“å‡ºå±‚\n",
    "<br />\n",
    "<img src=\"BERT.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "BERT-BiLSTM-CRFä¸€ä¸ªåºåˆ—æ ‡æ³¨çš„ç»å…¸ç½‘ç»œç»“æ„\n",
    "\n",
    "<img src=\"bert-bilstm-crf.jpg\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 9.2.2. Encoder-Decoder LMï¼Œæœºå™¨ç¿»è¯‘/æ–‡æœ¬ç”Ÿæˆï¼ˆå¤§è¯­è¨€æ¨¡å‹çš„ä¸€ç§å½¢æ€ï¼‰\n",
    "\n",
    "- Decoder ä¹Ÿæ˜¯ N å±‚ transformer ç»“æ„\n",
    "- ç”Ÿæˆä¸€ä¸ª tokenï¼ŒæŠŠå®ƒåŠ å…¥ä¸Šæ–‡ï¼Œå†ç”Ÿæˆä¸‹ä¸€ä¸ª tokenï¼Œä»¥æ­¤ç±»æ¨\n",
    "  <br />\n",
    "  <img src=\"decoder1.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "Decoder çš„æ¯ä¸ª token ä¸ encoder æœ€åä¸€å±‚çš„è¾“å‡ºå’Œ decoder ä¸Šæ–‡çš„ token ä¸€èµ·åš attention\n",
    "<br />\n",
    "<img src=\"decoder2.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "Decoder çš„ token åªèƒ½ attend åˆ°ä¸Šæ–‡çš„ tokenï¼ˆå› ä¸ºæ­¤æ—¶ä¸‹æ–‡è¿˜æ²¡æœ‰å‡ºç°ï¼‰\n",
    "<br />\n",
    "<img src=\"decoder3.png\" style=\"margin-left: 0px\" width=\"600px\">\n",
    "\n",
    "### 9.2.3. Decoder-Only LM ä¹Ÿå« Causal LM æˆ– Left-to-right LMï¼ˆGPT å®¶æ—ï¼‰\n",
    "\n",
    "<br />\n",
    "<img src=\"decoder4.png\" style=\"margin-left: 0px\" width=\"400px\">\n",
    "\n",
    "### 9.2.4. å¤§è¯­è¨€æ¨¡å‹æ—è°±\n",
    "\n",
    "<br />\n",
    "<img src=\"llm.jpg\" style=\"margin-left: 0px\" width=\"800px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ç»§ç»­](huggingface/index.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
